 The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.     

 [![logo](https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg) Back to arXiv](https://arxiv.org/)

[](https://arxiv.org/abs/2402.06557v1)[](javascript:toggleColorScheme() "Toggle dark/light mode")

 [![logo](https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg) Back to arXiv](https://arxiv.org/)

This is **experimental HTML** to improve accessibility. We invite you to report rendering errors. Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off. Learn more [about this project](https://info.arxiv.org/about/accessible_HTML.html) and [help improve conversions](https://info.arxiv.org/help/submit_latex_best_practices.html).

[Why HTML?](https://info.arxiv.org/about/accessible_HTML.html) [Report Issue](#myForm) [Back to Abstract](https://arxiv.org/abs/2402.06557v1) [Download PDF](https://arxiv.org/pdf/2402.06557v1)[](javascript:toggleColorScheme() "Toggle dark/light mode")

Table of Contents
-----------------

1.  [1 Contributions](https://arxiv.org/html/2402.06557v1#S1 "1 Contributions ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
2.  [2 Motivation](https://arxiv.org/html/2402.06557v1#S2 "2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [2.1 Large Language Models](https://arxiv.org/html/2402.06557v1#S2.SS1 "2.1 Large Language Models ‣ 2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Hallucinations](https://arxiv.org/html/2402.06557v1#S2.SS1.SSS0.Px1 "Hallucinations ‣ 2.1 Large Language Models ‣ 2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Reasoning](https://arxiv.org/html/2402.06557v1#S2.SS1.SSS0.Px2 "Reasoning ‣ 2.1 Large Language Models ‣ 2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [Planning](https://arxiv.org/html/2402.06557v1#S2.SS1.SSS0.Px3 "Planning ‣ 2.1 Large Language Models ‣ 2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [2.2 Cognitive Science](https://arxiv.org/html/2402.06557v1#S2.SS2 "2.2 Cognitive Science ‣ 2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
3.  [3 Background](https://arxiv.org/html/2402.06557v1#S3 "3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [3.1 First-Order Logic](https://arxiv.org/html/2402.06557v1#S3.SS1 "3.1 First-Order Logic ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Universal Quantification and Implication](https://arxiv.org/html/2402.06557v1#S3.SS1.SSSx2.Px1 "Universal Quantification and Implication ‣ Language and Deduction Rules ‣ 3.1 First-Order Logic ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Logical Connectives](https://arxiv.org/html/2402.06557v1#S3.SS1.SSSx2.Px2 "Logical Connectives ‣ Language and Deduction Rules ‣ 3.1 First-Order Logic ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [and](https://arxiv.org/html/2402.06557v1#S3.SS1.SSSx2.Px3 "and ‣ Language and Deduction Rules ‣ 3.1 First-Order Logic ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        4.  [or](https://arxiv.org/html/2402.06557v1#S3.SS1.SSSx2.Px4 "or ‣ Language and Deduction Rules ‣ 3.1 First-Order Logic ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        5.  [Negation](https://arxiv.org/html/2402.06557v1#S3.SS1.SSSx2.Px5 "Negation ‣ Language and Deduction Rules ‣ 3.1 First-Order Logic ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [3.2 Bayesian Networks](https://arxiv.org/html/2402.06557v1#S3.SS2 "3.2 Bayesian Networks ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Directed Acyclic Graph](https://arxiv.org/html/2402.06557v1#S3.SS2.SSSx4.Px1 "Directed Acyclic Graph ‣ Traditional Bayesian Networks ‣ 3.2 Bayesian Networks ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Complexity](https://arxiv.org/html/2402.06557v1#S3.SS2.SSSx4.Px2 "Complexity ‣ Traditional Bayesian Networks ‣ 3.2 Bayesian Networks ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
4.  [4 A Novel Calculus Over Semantic Roles](https://arxiv.org/html/2402.06557v1#S4 "4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [4.1 Motivation](https://arxiv.org/html/2402.06557v1#S4.SS1 "4.1 Motivation ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [4.2 Language Definition](https://arxiv.org/html/2402.06557v1#S4.SS2 "4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [A Key-Value Calculus](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px1 "A Key-Value Calculus ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Truth Values](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px2 "Truth Values ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [Entities](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px3 "Entities ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        4.  [Types](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px4 "Types ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        5.  [Constants](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px5 "Constants ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        6.  [Variables](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px6 "Variables ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        7.  [Function Names](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px7 "Function Names ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        8.  [Arguments](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px8 "Arguments ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        9.  [Role Labels](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px9 "Role Labels ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        10.  [Role Sets and Maps](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px10 "Role Sets and Maps ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        11.  [Predicates](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px11 "Predicates ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        12.  [Propositions](https://arxiv.org/html/2402.06557v1#S4.SS2.SSS0.Px12 "Propositions ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [4.3 Quantification and Implication](https://arxiv.org/html/2402.06557v1#S4.SS3 "4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [4.3.1 Statistical Inference](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS1 "4.3.1 Statistical Inference ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [4.3.2 Predicate Implication Links](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS2 "4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            1.  [Example](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS2.Px1 "Example ‣ 4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            2.  [Role Set Mapping](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS2.Px2 "Role Set Mapping ‣ 4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            3.  [Predicate Implication Link](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS2.Px3 "Predicate Implication Link ‣ 4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [4.3.3 Conjoined Predicate Implication](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS3 "4.3.3 Conjoined Predicate Implication ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            1.  [Motivation](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS3.Px1 "Motivation ‣ 4.3.3 Conjoined Predicate Implication ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            2.  [Formulation](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS3.Px2 "Formulation ‣ 4.3.3 Conjoined Predicate Implication ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
5.  [5 The Proposition Graph](https://arxiv.org/html/2402.06557v1#S5 "5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [5.1 Markov Assumption](https://arxiv.org/html/2402.06557v1#S5.SS1 "5.1 Markov Assumption ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [5.2 Lazy Graph Storage](https://arxiv.org/html/2402.06557v1#S5.SS2 "5.2 Lazy Graph Storage ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [The Full Graph is Unbounded](https://arxiv.org/html/2402.06557v1#S5.SS2.SSS0.Px1 "The Full Graph is Unbounded ‣ 5.2 Lazy Graph Storage ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Stored vs. Dynamically Calculated Probabilities](https://arxiv.org/html/2402.06557v1#S5.SS2.SSS0.Px2 "Stored vs. Dynamically Calculated Probabilities ‣ 5.2 Lazy Graph Storage ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [Use of the Markov Assumption](https://arxiv.org/html/2402.06557v1#S5.SS2.SSS0.Px3 "Use of the Markov Assumption ‣ 5.2 Lazy Graph Storage ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [5.3 Boolean Algebra](https://arxiv.org/html/2402.06557v1#S5.SS3 "5.3 Boolean Algebra ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    4.  [5.4 Bipartite Graph](https://arxiv.org/html/2402.06557v1#S5.SS4 "5.4 Bipartite Graph ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    5.  [5.5 Conjunction Nodes](https://arxiv.org/html/2402.06557v1#S5.SS5 "5.5 Conjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [5.5.1 Deterministic Definition](https://arxiv.org/html/2402.06557v1#S5.SS5.SSS1 "5.5.1 Deterministic Definition ‣ 5.5 Conjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [5.5.2 Higher-Level Features](https://arxiv.org/html/2402.06557v1#S5.SS5.SSS2 "5.5.2 Higher-Level Features ‣ 5.5 Conjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    6.  [5.6 Disjunction Nodes](https://arxiv.org/html/2402.06557v1#S5.SS6 "5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [5.6.1 Deterministic Definition](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS1 "5.6.1 Deterministic Definition ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [5.6.2 Learned Disjunctive Model](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS2 "5.6.2 Learned Disjunctive Model ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [5.6.3 The Similarity Between Disjunction and Linear Exponential](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS3 "5.6.3 The Similarity Between Disjunction and Linear Exponential ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        4.  [5.6.4 On the Use of a Linear Model](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS4 "5.6.4 On the Use of a Linear Model ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
6.  [6 The Implication Graph](https://arxiv.org/html/2402.06557v1#S6 "6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [6.1 Infinite Use of Finite Means](https://arxiv.org/html/2402.06557v1#S6.SS1 "6.1 Infinite Use of Finite Means ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [6.2 Graph Operations](https://arxiv.org/html/2402.06557v1#S6.SS2 "6.2 Graph Operations ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Construction](https://arxiv.org/html/2402.06557v1#S6.SS2.SSS0.Px1 "Construction ‣ 6.2 Graph Operations ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Backwards Links for a Predicate](https://arxiv.org/html/2402.06557v1#S6.SS2.SSS0.Px2 "Backwards Links for a Predicate ‣ 6.2 Graph Operations ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [6.3 Abstraction and Backwards Substitution](https://arxiv.org/html/2402.06557v1#S6.SS3 "6.3 Abstraction and Backwards Substitution ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Abstraction](https://arxiv.org/html/2402.06557v1#S6.SS3.SSS0.Px1 "Abstraction ‣ 6.3 Abstraction and Backwards Substitution ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Backwards Substitution](https://arxiv.org/html/2402.06557v1#S6.SS3.SSS0.Px2 "Backwards Substitution ‣ 6.3 Abstraction and Backwards Substitution ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    4.  [6.4 Proposition Factors and Contexts](https://arxiv.org/html/2402.06557v1#S6.SS4 "6.4 Proposition Factors and Contexts ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Proposition Factor](https://arxiv.org/html/2402.06557v1#S6.SS4.SSS0.Px1 "Proposition Factor ‣ 6.4 Proposition Factors and Contexts ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Proposition Factor Context](https://arxiv.org/html/2402.06557v1#S6.SS4.SSS0.Px2 "Proposition Factor Context ‣ 6.4 Proposition Factors and Contexts ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [Markov Assumption](https://arxiv.org/html/2402.06557v1#S6.SS4.SSS0.Px3 "Markov Assumption ‣ 6.4 Proposition Factors and Contexts ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    5.  [6.5 Inference-Time Proposition Graph Creation](https://arxiv.org/html/2402.06557v1#S6.SS5 "6.5 Inference-Time Proposition Graph Creation ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    6.  [6.6 Feature Function](https://arxiv.org/html/2402.06557v1#S6.SS6 "6.6 Feature Function ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
7.  [7 Inference](https://arxiv.org/html/2402.06557v1#S7 "7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [7.1 The Probability Query](https://arxiv.org/html/2402.06557v1#S7.SS1 "7.1 The Probability Query ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [7.2 Marginalization](https://arxiv.org/html/2402.06557v1#S7.SS2 "7.2 Marginalization ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [7.3 Iterative Belief Propagation](https://arxiv.org/html/2402.06557v1#S7.SS3 "7.3 Iterative Belief Propagation ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    4.  [7.4 Message Passing Calculations](https://arxiv.org/html/2402.06557v1#S7.SS4 "7.4 Message Passing Calculations ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Notation](https://arxiv.org/html/2402.06557v1#S7.SS4.SSS0.Px1 "Notation ‣ 7.4 Message Passing Calculations ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Computations](https://arxiv.org/html/2402.06557v1#S7.SS4.SSS0.Px2 "Computations ‣ 7.4 Message Passing Calculations ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [Values](https://arxiv.org/html/2402.06557v1#S7.SS4.SSS0.Px3 "Values ‣ 7.4 Message Passing Calculations ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        4.  [Messages](https://arxiv.org/html/2402.06557v1#S7.SS4.SSS0.Px4 "Messages ‣ 7.4 Message Passing Calculations ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
8.  [8 Experiments](https://arxiv.org/html/2402.06557v1#S8 "8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [8.1 Logical Structures](https://arxiv.org/html/2402.06557v1#S8.SS1 "8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [8.1.1 Method](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS1 "8.1.1 Method ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            1.  [Synthetic Data](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS1.Px1 "Synthetic Data ‣ 8.1.1 Method ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            2.  [Example Universe](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS1.Px2 "Example Universe ‣ 8.1.1 Method ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            3.  [Training](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS1.Px3 "Training ‣ 8.1.1 Method ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            4.  [Belief Propagation Convergence](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS1.Px4 "Belief Propagation Convergence ‣ 8.1.1 Method ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [8.1.2 Results](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS2 "8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            1.  [No Evidence](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS2.Px1 "No Evidence ‣ 8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            2.  [Forward Only](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS2.Px2 "Forward Only ‣ 8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            3.  [Forward and Backward](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS2.Px3 "Forward and Backward ‣ 8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
            4.  [Backward Only](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS2.Px4 "Backward Only ‣ 8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [8.2 Message Propagation](https://arxiv.org/html/2402.06557v1#S8.SS2 "8.2 Message Propagation ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Method](https://arxiv.org/html/2402.06557v1#S8.SS2.SSS0.Px1 "Method ‣ 8.2 Message Propagation ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Results](https://arxiv.org/html/2402.06557v1#S8.SS2.SSS0.Px2 "Results ‣ 8.2 Message Propagation ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
9.  [9 Complexity of Inference](https://arxiv.org/html/2402.06557v1#S9 "9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [9.1 Provably Exact Inference](https://arxiv.org/html/2402.06557v1#S9.SS1 "9.1 Provably Exact Inference ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [9.2 Empirically Successful Iterative Belief Propagation](https://arxiv.org/html/2402.06557v1#S9.SS2 "9.2 Empirically Successful Iterative Belief Propagation ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [9.3 Faster Disjunction](https://arxiv.org/html/2402.06557v1#S9.SS3 "9.3 Faster Disjunction ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [Overview](https://arxiv.org/html/2402.06557v1#S9.SS3.SSS0.Px1 "Overview ‣ 9.3 Faster Disjunction ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [Importance Sampling](https://arxiv.org/html/2402.06557v1#S9.SS3.SSS0.Px2 "Importance Sampling ‣ 9.3 Faster Disjunction ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [Linear Time Disjunction](https://arxiv.org/html/2402.06557v1#S9.SS3.SSS0.Px3 "Linear Time Disjunction ‣ 9.3 Faster Disjunction ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    4.  [9.4 Faster Conjunction](https://arxiv.org/html/2402.06557v1#S9.SS4 "9.4 Faster Conjunction ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
10.  [10 Compared to Other Logical Models](https://arxiv.org/html/2402.06557v1#S10 "10 Compared to Other Logical Models ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [AlphaGeometry](https://arxiv.org/html/2402.06557v1#S10.SS0.SSS0.Px1 "AlphaGeometry ‣ 10 Compared to Other Logical Models ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [Self-Discover](https://arxiv.org/html/2402.06557v1#S10.SS0.SSS0.Px2 "Self-Discover ‣ 10 Compared to Other Logical Models ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
11.  [11 Implementation](https://arxiv.org/html/2402.06557v1#S11 "11 Implementation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
12.  [12 Future Work](https://arxiv.org/html/2402.06557v1#S12 "12 Future Work ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [Learning from Unlabeled Text](https://arxiv.org/html/2402.06557v1#S12.SS0.SSS0.Px1 "Learning from Unlabeled Text ‣ 12 Future Work ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [Belief Propagation](https://arxiv.org/html/2402.06557v1#S12.SS0.SSS0.Px2 "Belief Propagation ‣ 12 Future Work ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [Logical Language Features](https://arxiv.org/html/2402.06557v1#S12.SS0.SSS0.Px3 "Logical Language Features ‣ 12 Future Work ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")

Report issue for preceding element

HTML conversions [sometimes display errors](https://info.dev.arxiv.org/about/accessibility_html_error_messages.html) due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.

Report issue for preceding element

*   failed: inconsolata
*   failed: tikz-dependency

Authors: achieve the best HTML results from your LaTeX submissions by following these [best practices](https://info.arxiv.org/help/submit_latex_best_practices.html).

Report issue for preceding element

License: CC BY 4.0

arXiv:2402.06557v1 \[cs.AI\] 09 Feb 2024

function closePopup() { document.querySelector('.package-alerts').style.display = 'none'; }

The Quantified Boolean Bayesian Network  
Theory and Experiments with a Logical Graphical Model ††thanks: The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.
==================================================================================================================================================================================================================================================

Report issue for preceding element

Greg Coppola  
coppola.ai  
Research. Develop. Meme

Report issue for preceding element

(February 11, 2024)

###### Contents

Report issue for preceding element

1.  [1 Contributions](https://arxiv.org/html/2402.06557v1#S1 "1 Contributions ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
2.  [2 Motivation](https://arxiv.org/html/2402.06557v1#S2 "2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [2.1 Large Language Models](https://arxiv.org/html/2402.06557v1#S2.SS1 "2.1 Large Language Models ‣ 2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [2.2 Cognitive Science](https://arxiv.org/html/2402.06557v1#S2.SS2 "2.2 Cognitive Science ‣ 2 Motivation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
3.  [3 Background](https://arxiv.org/html/2402.06557v1#S3 "3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [3.1 First-Order Logic](https://arxiv.org/html/2402.06557v1#S3.SS1 "3.1 First-Order Logic ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [3.2 Bayesian Networks](https://arxiv.org/html/2402.06557v1#S3.SS2 "3.2 Bayesian Networks ‣ 3 Background ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
4.  [4 A Novel Calculus Over Semantic Roles](https://arxiv.org/html/2402.06557v1#S4 "4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [4.1 Motivation](https://arxiv.org/html/2402.06557v1#S4.SS1 "4.1 Motivation ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [4.2 Language Definition](https://arxiv.org/html/2402.06557v1#S4.SS2 "4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [4.3 Quantification and Implication](https://arxiv.org/html/2402.06557v1#S4.SS3 "4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [4.3.1 Statistical Inference](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS1 "4.3.1 Statistical Inference ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [4.3.2 Predicate Implication Links](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS2 "4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [4.3.3 Conjoined Predicate Implication](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS3 "4.3.3 Conjoined Predicate Implication ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
5.  [5 The Proposition Graph](https://arxiv.org/html/2402.06557v1#S5 "5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [5.1 Markov Assumption](https://arxiv.org/html/2402.06557v1#S5.SS1 "5.1 Markov Assumption ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [5.2 Lazy Graph Storage](https://arxiv.org/html/2402.06557v1#S5.SS2 "5.2 Lazy Graph Storage ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [5.3 Boolean Algebra](https://arxiv.org/html/2402.06557v1#S5.SS3 "5.3 Boolean Algebra ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    4.  [5.4 Bipartite Graph](https://arxiv.org/html/2402.06557v1#S5.SS4 "5.4 Bipartite Graph ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    5.  [5.5 Conjunction Nodes](https://arxiv.org/html/2402.06557v1#S5.SS5 "5.5 Conjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [5.5.1 Deterministic Definition](https://arxiv.org/html/2402.06557v1#S5.SS5.SSS1 "5.5.1 Deterministic Definition ‣ 5.5 Conjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [5.5.2 Higher-Level Features](https://arxiv.org/html/2402.06557v1#S5.SS5.SSS2 "5.5.2 Higher-Level Features ‣ 5.5 Conjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    6.  [5.6 Disjunction Nodes](https://arxiv.org/html/2402.06557v1#S5.SS6 "5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [5.6.1 Deterministic Definition](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS1 "5.6.1 Deterministic Definition ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [5.6.2 Learned Disjunctive Model](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS2 "5.6.2 Learned Disjunctive Model ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        3.  [5.6.3 The Similarity Between Disjunction and Linear Exponential](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS3 "5.6.3 The Similarity Between Disjunction and Linear Exponential ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        4.  [5.6.4 On the Use of a Linear Model](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS4 "5.6.4 On the Use of a Linear Model ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
6.  [6 The Implication Graph](https://arxiv.org/html/2402.06557v1#S6 "6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [6.1 Infinite Use of Finite Means](https://arxiv.org/html/2402.06557v1#S6.SS1 "6.1 Infinite Use of Finite Means ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [6.2 Graph Operations](https://arxiv.org/html/2402.06557v1#S6.SS2 "6.2 Graph Operations ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [6.3 Abstraction and Backwards Substitution](https://arxiv.org/html/2402.06557v1#S6.SS3 "6.3 Abstraction and Backwards Substitution ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    4.  [6.4 Proposition Factors and Contexts](https://arxiv.org/html/2402.06557v1#S6.SS4 "6.4 Proposition Factors and Contexts ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    5.  [6.5 Inference-Time Proposition Graph Creation](https://arxiv.org/html/2402.06557v1#S6.SS5 "6.5 Inference-Time Proposition Graph Creation ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    6.  [6.6 Feature Function](https://arxiv.org/html/2402.06557v1#S6.SS6 "6.6 Feature Function ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
7.  [7 Inference](https://arxiv.org/html/2402.06557v1#S7 "7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [7.1 The Probability Query](https://arxiv.org/html/2402.06557v1#S7.SS1 "7.1 The Probability Query ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [7.2 Marginalization](https://arxiv.org/html/2402.06557v1#S7.SS2 "7.2 Marginalization ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [7.3 Iterative Belief Propagation](https://arxiv.org/html/2402.06557v1#S7.SS3 "7.3 Iterative Belief Propagation ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    4.  [7.4 Message Passing Calculations](https://arxiv.org/html/2402.06557v1#S7.SS4 "7.4 Message Passing Calculations ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
8.  [8 Experiments](https://arxiv.org/html/2402.06557v1#S8 "8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [8.1 Logical Structures](https://arxiv.org/html/2402.06557v1#S8.SS1 "8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        1.  [8.1.1 Method](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS1 "8.1.1 Method ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
        2.  [8.1.2 Results](https://arxiv.org/html/2402.06557v1#S8.SS1.SSS2 "8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [8.2 Message Propagation](https://arxiv.org/html/2402.06557v1#S8.SS2 "8.2 Message Propagation ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
9.  [9 Complexity of Inference](https://arxiv.org/html/2402.06557v1#S9 "9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    1.  [9.1 Provably Exact Inference](https://arxiv.org/html/2402.06557v1#S9.SS1 "9.1 Provably Exact Inference ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    2.  [9.2 Empirically Successful Iterative Belief Propagation](https://arxiv.org/html/2402.06557v1#S9.SS2 "9.2 Empirically Successful Iterative Belief Propagation ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    3.  [9.3 Faster Disjunction](https://arxiv.org/html/2402.06557v1#S9.SS3 "9.3 Faster Disjunction ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
    4.  [9.4 Faster Conjunction](https://arxiv.org/html/2402.06557v1#S9.SS4 "9.4 Faster Conjunction ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
10.  [10 Compared to Other Logical Models](https://arxiv.org/html/2402.06557v1#S10 "10 Compared to Other Logical Models ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
11.  [11 Implementation](https://arxiv.org/html/2402.06557v1#S11 "11 Implementation ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")
12.  [12 Future Work](https://arxiv.org/html/2402.06557v1#S12 "12 Future Work ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")

1 Contributions
---------------

Report issue for preceding element

We introduce the Quantified Boolean Bayesian Network, QBBN for short, a model from the Bayesian Network family \[[Pearl, 1988](https://arxiv.org/html/2402.06557v1#bib.bibx31), [Neapolitan, 2003](https://arxiv.org/html/2402.06557v1#bib.bibx30)\], constructed and analyzed to provide a unified theory of logical and statistical reasoning. In particular, our work makes the following contributions:

Report issue for preceding element

*   •
    
    Unified Model of Logical and Probabilistic Reasoning  
    We provide a single data structure, the QBBN, which can do both:
    
    Report issue for preceding element
    
    *   –
        
        Statistical Reasoning – The QBBN is a graphical model that can answer probabilistic queries \[[Koller and Friedman, 2009](https://arxiv.org/html/2402.06557v1#bib.bibx20)\], i.e. for information retrieval \[[Shannon, 1948](https://arxiv.org/html/2402.06557v1#bib.bibx40)\].
        
        Report issue for preceding element
        
    *   –
        
        Logical Reasoning – We show how the QBBN fits precisely into a larger consistent and complete logical deduction system \[[Gentzen, 1934](https://arxiv.org/html/2402.06557v1#bib.bibx13)\] for the first-order calculus \[[Frege, 1879](https://arxiv.org/html/2402.06557v1#bib.bibx12)\].
        
        Report issue for preceding element
        
    
    The completeness proof is outlined in \[[Coppola, 2024a](https://arxiv.org/html/2402.06557v1#bib.bibx7)\].
    
    Report issue for preceding element
    
*   •
    
    A Generative Model Without Hallucinations  
    The QBBN shows how to create a generative model of the (latent logical forms underlying) unlabeled text. Like the large language model \[[Bahdanau et al., 2014](https://arxiv.org/html/2402.06557v1#bib.bibx1), [Vaswani et al., 2017](https://arxiv.org/html/2402.06557v1#bib.bibx49), [Radford et al., 2018](https://arxiv.org/html/2402.06557v1#bib.bibx35)\], the QBBN is generative, and so can be used to compress the data \[[Sutskever, 2023](https://arxiv.org/html/2402.06557v1#bib.bibx44)\]. But, the QBBN does not hallucinate. It reasons consistently (i.e., ensuring that P⁢(x)+P⁢(¬⁢x)\=1𝑃𝑥𝑃𝑥1P(x)+P(\\neg x)=1italic\_P ( italic\_x ) + italic\_P ( ¬ italic\_x ) = 1 for all questions x𝑥xitalic\_x), and can explain its reasoning in terms of causality, like any Bayesian Network can.
    
    Report issue for preceding element
    
*   •
    
    Very Efficient Bayesian Inference  
    In general, inference in a Bayesian Network is intractable, i.e. Ω⁢(2N)Ωsuperscript2𝑁\\Omega(2^{N})roman\_Ω ( 2 start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT ) for N𝑁Nitalic\_N random variables \[[Neapolitan, 2003](https://arxiv.org/html/2402.06557v1#bib.bibx30)\]. Our division of Bayesian Network nodes into and and or boolean gates, along with our use of the unguaranteed but empirically converging iterative belief propagation \[[Murphy et al., 1999](https://arxiv.org/html/2402.06557v1#bib.bibx29), [Smith and Eisner, 2008](https://arxiv.org/html/2402.06557v1#bib.bibx41)\] means that inference can now be not only tractable, but very efficient, with one full pass of approximate belief propagation requiring only time O⁢(N⁢2n)𝑂𝑁superscript2𝑛O(N2^{n})italic\_O ( italic\_N 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ), where N𝑁Nitalic\_N is the number of network variables involved, and n𝑛nitalic\_n bounds the number of incoming connections in any and or or gate. Moreover, we discuss why it may be possible to bring the factor computation cost to O⁢(n)𝑂𝑛O(n)italic\_O ( italic\_n ) instead of O⁢(2n)𝑂superscript2𝑛O(2^{n})italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ) for each of and and or.
    
    Report issue for preceding element
    
*   •
    
    Fast Versus Slow Thinking  
    We give, to our knowledge, the first mathematical explanation of the distinction between what has come to be known as fast versus slow thinking \[[Kahneman, 2011](https://arxiv.org/html/2402.06557v1#bib.bibx19)\]. This explanation is based on proof theory of the natural deduction calculus, and accords both with our graphical formulation, as well human experience. As a special case of general reasoning, we analyze planning, which task \[[LeCun, 2023](https://arxiv.org/html/2402.06557v1#bib.bibx21)\] has argued LLM’s do not properly support. While \[[Coppola, 2024a](https://arxiv.org/html/2402.06557v1#bib.bibx7)\] contains the fast versus slow analysis, this work contains the related model details.
    
    Report issue for preceding element
    
*   •
    
    Calculus Over Dependency Trees  
    Empirically, labeled dependnecy trees \[[Eisner, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx11)\] are the easiest syntactic formalism to parse to. Traditionally, parsing language to a complete and consistent calculus required using the first-order logic calculus \[[Steedman, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx42)\], but translation to literally first-order calculus, requires an unecessary imposition of positional order on arguments that is not helpful for knowledge encoding. By defining a complete calculus closer to the key-value labeled dependency structure, we find it is easier to encode knowledge, and we minimize the distance between the surface form and the interpretation.
    
    Report issue for preceding element
    

2 Motivation
------------

Report issue for preceding element

### 2.1 Large Language Models

Report issue for preceding element

The Quantified Boolean Bayesian Network is introduced as a remedy the following drawbacks of the large language model \[[Bahdanau et al., 2014](https://arxiv.org/html/2402.06557v1#bib.bibx1), [Sutskever et al., 2014](https://arxiv.org/html/2402.06557v1#bib.bibx46), [Vaswani et al., 2017](https://arxiv.org/html/2402.06557v1#bib.bibx49), [Devlin et al., 2018](https://arxiv.org/html/2402.06557v1#bib.bibx10), [Radford et al., 2018](https://arxiv.org/html/2402.06557v1#bib.bibx35)\].

Report issue for preceding element

##### Hallucinations

Report issue for preceding element

While the large language model is widly popular for its ability to learn complex kinds of knowledge and even some reasoning from unlabeled text, the primary empirical user complaint with large language models is that of hallucinations \[[Sutskever and Huang, 2023](https://arxiv.org/html/2402.06557v1#bib.bibx45)\]. That is, a large language model can return answers that are not “supported by the training set” when judged by a human evaluator. This lack of reliability greatly limits throughput, because it requires all output of a large language model to be double-checked by the user.

Report issue for preceding element

##### Reasoning

Report issue for preceding element

Another noted problem is that the LLM does not reason logically, and does not have a logically consistent world view \[[Steedman, 2022](https://arxiv.org/html/2402.06557v1#bib.bibx43), [Hinton, 2023](https://arxiv.org/html/2402.06557v1#bib.bibx17)\]. We propose that these two problems with large language models are directly related. That is, the fact that a large language model will return answers unsupported by the training set is because of the fact that the large language model does not understand causality. If a knowledge data structure were able to explain its reasoning, and only return answers based on valid reasoning, then it would be unabled to return answers unsupported by the training set, and thus unable to hallucinate.

Report issue for preceding element

##### Planning

Report issue for preceding element

\[[LeCun, 2023](https://arxiv.org/html/2402.06557v1#bib.bibx21)\] has noted that one problem with large language models is that they do not seem to plan properly. We propose to understand the “complete” set of deduction rules as those in \[[Prawitz, 1965](https://arxiv.org/html/2402.06557v1#bib.bibx34)\], and from this perspective we can analyse planning as simple inferences (see \[[Coppola, 2024a](https://arxiv.org/html/2402.06557v1#bib.bibx7)\] and work in preparation), along with ∨\\vee∨\-elimination. This is to say, planning is a mix of forward inference, along with reasoning by cases, and this is a precisely simpler form of reasoning than general reasoning, because it does not use all the rules of general theorem-proving.

Report issue for preceding element

### 2.2 Cognitive Science

Report issue for preceding element

We are also interested in understanding the human mind and human reasoning. \[[Chomsky, 1957](https://arxiv.org/html/2402.06557v1#bib.bibx4)\] proposed to look for a universal grammar underlying all the diversity of human language. In some sense, the logical language underlying surface form, may be the only true universal language \[[Montague, 1973](https://arxiv.org/html/2402.06557v1#bib.bibx28), [Steedman, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx42)\]. So, understanding this logical language and how it interacts with both logical and probabilistic reasoning is of central concern for those interested in cognitive science.

Report issue for preceding element

3 Background
------------

Report issue for preceding element

### 3.1 First-Order Logic

Report issue for preceding element

#### Explanatory Power

Report issue for preceding element

In the philosophy of science it is by now taken for granted that all of mathematics and science can be expressed in terms of first-order logic (or its extensions) (see, e.g., \[[Pelletier, 2000](https://arxiv.org/html/2402.06557v1#bib.bibx32)\], and the references therein). Thus, we say that first-order logic is sufficient to model human reasoning. Extensions include second-order logic and modal logic \[[Prawitz, 1965](https://arxiv.org/html/2402.06557v1#bib.bibx34)\], but we leave this for future work, and focus on the first-order logic for simplicity.

Report issue for preceding element

#### Language and Deduction Rules

Report issue for preceding element

##### Universal Quantification and Implication

Report issue for preceding element

The method of universal quantification is represented by ∀for-all\\forall∀, and implication is represented by the →→\\rightarrow→ symbol. These two work crucially together, as in expressing Socrates’ classic syllogism that all men are mortal:

Report issue for preceding element

∀𝐱,m⁢a⁢n⁢(𝐱)→m⁢o⁢r⁢t⁢a⁢l⁢(𝐱)→for-all𝐱𝑚𝑎𝑛𝐱𝑚𝑜𝑟𝑡𝑎𝑙𝐱\\forall{\\bf x},man({\\bf x})\\rightarrow mortal({\\bf x})∀ bold\_x , italic\_m italic\_a italic\_n ( bold\_x ) → italic\_m italic\_o italic\_r italic\_t italic\_a italic\_l ( bold\_x )

(1)

This single rule licenses an unbounded number of inferences. For example, m⁢a⁢n⁢(𝐜j⁢a⁢c⁢k)𝑚𝑎𝑛subscript𝐜𝑗𝑎𝑐𝑘man({\\bf c}\_{jack})italic\_m italic\_a italic\_n ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ), we can conclude m⁢o⁢r⁢t⁢a⁢l⁢(𝐜j⁢a⁢c⁢k)𝑚𝑜𝑟𝑡𝑎𝑙subscript𝐜𝑗𝑎𝑐𝑘mortal({\\bf c}\_{jack})italic\_m italic\_o italic\_r italic\_t italic\_a italic\_l ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ), and if m⁢a⁢n⁢(𝐜a⁢r⁢j⁢u⁢n)𝑚𝑎𝑛subscript𝐜𝑎𝑟𝑗𝑢𝑛man({\\bf c}\_{arjun})italic\_m italic\_a italic\_n ( bold\_c start\_POSTSUBSCRIPT italic\_a italic\_r italic\_j italic\_u italic\_n end\_POSTSUBSCRIPT ) we can conclude m⁢o⁢r⁢t⁢a⁢l⁢(𝐜a⁢r⁢j⁢u⁢n)𝑚𝑜𝑟𝑡𝑎𝑙subscript𝐜𝑎𝑟𝑗𝑢𝑛mortal({\\bf c}\_{arjun})italic\_m italic\_o italic\_r italic\_t italic\_a italic\_l ( bold\_c start\_POSTSUBSCRIPT italic\_a italic\_r italic\_j italic\_u italic\_n end\_POSTSUBSCRIPT ), etc. This is how in language we make infinite use of finite means, as discussed in Section [6.1](https://arxiv.org/html/2402.06557v1#S6.SS1 "6.1 Infinite Use of Finite Means ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

##### Logical Connectives

Report issue for preceding element

There are two logical connectives designated as boolean in our system, corresponding to the two operations generally in a boolean algebra.

Report issue for preceding element

##### and

Report issue for preceding element

The first connective is and, represented with ∧\\wedge∧, as in:

Report issue for preceding element

m⁢a⁢n⁢(𝐜j⁢a⁢c⁢k)∧m⁢o⁢r⁢t⁢a⁢l⁢(𝐜j⁢a⁢c⁢k)𝑚𝑎𝑛subscript𝐜𝑗𝑎𝑐𝑘𝑚𝑜𝑟𝑡𝑎𝑙subscript𝐜𝑗𝑎𝑐𝑘man({\\bf c}\_{jack})\\wedge mortal({\\bf c}\_{jack})italic\_m italic\_a italic\_n ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) ∧ italic\_m italic\_o italic\_r italic\_t italic\_a italic\_l ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT )

(2)

This means that both m⁢a⁢n⁢(𝐜j⁢a⁢c⁢k)𝑚𝑎𝑛subscript𝐜𝑗𝑎𝑐𝑘man({\\bf c}\_{jack})italic\_m italic\_a italic\_n ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) and m⁢o⁢r⁢t⁢a⁢l⁢(𝐜j⁢a⁢c⁢k)𝑚𝑜𝑟𝑡𝑎𝑙subscript𝐜𝑗𝑎𝑐𝑘mortal({\\bf c}\_{jack})italic\_m italic\_o italic\_r italic\_t italic\_a italic\_l ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) are true.

Report issue for preceding element

##### or

Report issue for preceding element

The second connective is or, represented with ∨\\vee∨, as in:

Report issue for preceding element

m⁢a⁢n⁢(𝐜j⁢a⁢c⁢k)∧m⁢o⁢r⁢t⁢a⁢l⁢(𝐜j⁢a⁢c⁢k)𝑚𝑎𝑛subscript𝐜𝑗𝑎𝑐𝑘𝑚𝑜𝑟𝑡𝑎𝑙subscript𝐜𝑗𝑎𝑐𝑘man({\\bf c}\_{jack})\\wedge mortal({\\bf c}\_{jack})italic\_m italic\_a italic\_n ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) ∧ italic\_m italic\_o italic\_r italic\_t italic\_a italic\_l ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT )

(3)

This means that at least one of the terms is true, maybe both.

Report issue for preceding element

##### Negation

Report issue for preceding element

The negation of a statement is represented in most logical presentations using the symbol ¬\\neg¬. For example to say God is not mortal we can write ¬⁢m⁢o⁢r⁢t⁢a⁢l⁢(𝐜G⁢o⁢d)𝑚𝑜𝑟𝑡𝑎𝑙subscript𝐜𝐺𝑜𝑑\\neg mortal({\\bf c}\_{God})¬ italic\_m italic\_o italic\_r italic\_t italic\_a italic\_l ( bold\_c start\_POSTSUBSCRIPT italic\_G italic\_o italic\_d end\_POSTSUBSCRIPT ). In our network, the concept of negation plays a crucial role, but there is no specific junction for negation, because each boolean variable represents a probability both for true and false.

Report issue for preceding element

#### Completeness and Consistency

Report issue for preceding element

For any logical calculus, we have a notion of what is provable in that calculus. This is evaluated against a model interpretation, that says what is true. A logic is consistent if whatever is provable is true. A logic is completeness if whatever is true is provable. \[[Gödel, 1930](https://arxiv.org/html/2402.06557v1#bib.bibx15)\] proved the consistency and completeness of first-order calculus. A fundamental insight of this work is that, we are free to work in a more practical formalism, i.e. a graphical statistical model over semantic roles, than the first-order logic if we will only prove the consistency and completeness of this new logic, which we outline in \[[Coppola, 2024a](https://arxiv.org/html/2402.06557v1#bib.bibx7)\] and work in preparation.

Report issue for preceding element

### 3.2 Bayesian Networks

Report issue for preceding element

#### Markov Graphical Models

Report issue for preceding element

A distribution P⁢(\[𝐩1,…,𝐩N\])𝑃subscript𝐩1…subscript𝐩𝑁P(\[{\\bf p}\_{1},...,{\\bf p}\_{N}\])italic\_P ( \[ bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_p start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT \] ) factorizes according to a factor graph GFsubscript𝐺𝐹G\_{F}italic\_G start\_POSTSUBSCRIPT italic\_F end\_POSTSUBSCRIPT if there exists a set of factors {α}Fsubscript𝛼𝐹\\left\\{\\alpha\\right\\}\_{F}{ italic\_α } start\_POSTSUBSCRIPT italic\_F end\_POSTSUBSCRIPT and factor functions ΨαsubscriptΨ𝛼\\Psi\_{\\alpha}roman\_Ψ start\_POSTSUBSCRIPT italic\_α end\_POSTSUBSCRIPT such that P⁢(\[𝐩1,…,𝐩N\])𝑃subscript𝐩1…subscript𝐩𝑁P(\[{\\bf p}\_{1},...,{\\bf p}\_{N}\])italic\_P ( \[ bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_p start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT \] ) can be written as \[[Sutton and McCallum, 2011](https://arxiv.org/html/2402.06557v1#bib.bibx47)\]:

Report issue for preceding element

P⁢(\[𝐩1,…,𝐩N\])\=Z−1⁢∏α∈FΨα⁢({𝐩}α)𝑃subscript𝐩1…subscript𝐩𝑁superscript𝑍1subscriptproduct𝛼𝐹subscriptΨ𝛼subscript𝐩𝛼P(\[{\\bf p}\_{1},...,{\\bf p}\_{N}\])=Z^{-1}\\prod\_{\\alpha\\in F}\\Psi\_{\\alpha}(\\left% \\{{\\bf p}\\right\\}\_{\\alpha})italic\_P ( \[ bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_p start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT \] ) = italic\_Z start\_POSTSUPERSCRIPT - 1 end\_POSTSUPERSCRIPT ∏ start\_POSTSUBSCRIPT italic\_α ∈ italic\_F end\_POSTSUBSCRIPT roman\_Ψ start\_POSTSUBSCRIPT italic\_α end\_POSTSUBSCRIPT ( { bold\_p } start\_POSTSUBSCRIPT italic\_α end\_POSTSUBSCRIPT )

(4)

Here, {𝐩}αsubscript𝐩𝛼\\left\\{{\\bf p}\\right\\}\_{\\alpha}{ bold\_p } start\_POSTSUBSCRIPT italic\_α end\_POSTSUBSCRIPT are the set of all variables 𝐩𝐩{\\bf p}bold\_p in the factor α𝛼\\alphaitalic\_α and Z𝑍Zitalic\_Z is a normalization constant that ensures that the probabilities sum to one. Doing normalization, and relatedly marginalization, in a general graphical model takes time Ω⁢(2N)Ωsuperscript2𝑁\\Omega(2^{N})roman\_Ω ( 2 start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT ).

Report issue for preceding element

#### Boolean Network

Report issue for preceding element

The QBBN is deliberately formulated as a boolean network, in which all propositional variables 𝐩𝐩{\\bf p}bold\_p are modeled as either taking the value true, represented by 1111, or false, represented by 00. Note that, while P⁢(𝐩\=z)𝑃𝐩𝑧P({\\bf p}=z)italic\_P ( bold\_p = italic\_z ) is a probability, for z∈{0,1}𝑧01z\\in\\left\\{0,1\\right\\}italic\_z ∈ { 0 , 1 }, the possible values that 𝐩𝐩{\\bf p}bold\_p can take are boolean.

Report issue for preceding element

#### Markov Logic Network

Report issue for preceding element

\[[Richardson and Domingos, 2006](https://arxiv.org/html/2402.06557v1#bib.bibx37)\] use a graphical boolean statistical network to score sentences constrained by the deductions of the first-order calculus. Inference in Markov Networks in general is #P-complete \[[Roth, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx38)\], which is Ω⁢(2N)Ωsuperscript2𝑁\\Omega(2^{N})roman\_Ω ( 2 start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT ). Because exact inference is intractable, \[[Richardson and Domingos, 2006](https://arxiv.org/html/2402.06557v1#bib.bibx37)\] use approximate inference via Markov Chain Monte Carlo \[[Gilks et al., 1996](https://arxiv.org/html/2402.06557v1#bib.bibx14)\] sampling. We propose a model to a similar effect, but with a large improvement in run-time through the use of unguaranteed belief propagation. This is useful because of the number of daily inferences that are currently demanded of large language models.

Report issue for preceding element

#### Traditional Bayesian Networks

Report issue for preceding element

##### Directed Acyclic Graph

Report issue for preceding element

A traditional Bayesian Network is a directed graphical model, where each factor maps α𝛼\\alphaitalic\_α input variables 𝐚isubscript𝐚𝑖{\\bf a}\_{i}bold\_a start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT to an output variable 𝐳𝐳{\\bf z}bold\_z:

Report issue for preceding element

Ψ⁢(𝐳|𝐚1,…,𝐚n)Ψconditional𝐳subscript𝐚1…subscript𝐚𝑛\\Psi({\\bf z}\\ |\\ {\\bf a}\_{1},...,{\\bf a}\_{n})roman\_Ψ ( bold\_z | bold\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_a start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT )

(5)

For each pair (𝐳,𝐚)𝐳𝐚({\\bf z},{\\bf a})( bold\_z , bold\_a ), we will refer to 𝐳𝐳{\\bf z}bold\_z as the child (or conclusion), and to 𝐚𝐚{\\bf a}bold\_a a the parent (or assumption). The directed nature of the factor gives rise to two clear inference directions: forwards, in which information passes from causes to effects, and backwards, in which information passes from effects (the observations), backwards to causes (a hypothesis).

Report issue for preceding element

##### Complexity

Report issue for preceding element

Inference in general Bayesian Networks is also #P-complete \[[Cooper, 1990](https://arxiv.org/html/2402.06557v1#bib.bibx6)\], and even NP-hard to provably approximate \[[Roth, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx38)\]. The difficulty is owing to the difficulty of marginalizing over undirected cycles in the factor graph \[[Neapolitan, 2003](https://arxiv.org/html/2402.06557v1#bib.bibx30), [Koller and Friedman, 2009](https://arxiv.org/html/2402.06557v1#bib.bibx20)\]. Nevertheless, loopy belief propagation, which we will henceforth call iterative belief propagation, while not provably convergent, has been found to empirically to converge in many situations \[[Murphy et al., 1999](https://arxiv.org/html/2402.06557v1#bib.bibx29), [Smith and Eisner, 2008](https://arxiv.org/html/2402.06557v1#bib.bibx41)\]. The complexity of belief propagation is discussed in detail in Section [9](https://arxiv.org/html/2402.06557v1#S9 "9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

#### Quantification in Bayesian Networks

Report issue for preceding element

An analog of universal quantification has been studied under the rubric of plate models \[[Koller and Friedman, 2009](https://arxiv.org/html/2402.06557v1#bib.bibx20)\], in which nodes sharing a template structure can share weights. We also employ this parameter sharing, but view it instead from a logical perspective as quantification.

Report issue for preceding element

4 A Novel Calculus Over Semantic Roles
--------------------------------------

Report issue for preceding element

### 4.1 Motivation

Report issue for preceding element

We have said that the calculus of first-order logic is complete, consistent, and sufficient for expressing mathematics and science. However, the language of the first-order logic logic is quite far from the labeled dependency parses that are most easily parsed to \[[Eisner, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx11), [McDonald et al., 2005](https://arxiv.org/html/2402.06557v1#bib.bibx25), [Zhang and Nivre, 2011](https://arxiv.org/html/2402.06557v1#bib.bibx51)\]. For example, \[[Lewis and Steedman, 2013](https://arxiv.org/html/2402.06557v1#bib.bibx23)\] shows how the sentence Shakespeare wrote Macbeth can be translated via a system of functional categories to a first-order language formula:

Report issue for preceding element

w⁢r⁢o⁢t⁢ea⁢r⁢g0:per,a⁢r⁢g1:book⁢(𝐜S⁢h⁢a⁢k⁢e⁢s⁢p⁢a⁢r⁢e,𝐜M⁢a⁢c⁢b⁢e⁢t⁢h)𝑤𝑟𝑜𝑡subscript𝑒:𝑎𝑟subscript𝑔0per𝑎𝑟subscript𝑔1:booksubscript𝐜𝑆ℎ𝑎𝑘𝑒𝑠𝑝𝑎𝑟𝑒subscript𝐜𝑀𝑎𝑐𝑏𝑒𝑡ℎwrote\_{arg\_{0}:\\textsc{per},arg\_{1}:\\textsc{book}}({\\bf c}\_{Shakespare},{\\bf c% }\_{Macbeth})italic\_w italic\_r italic\_o italic\_t italic\_e start\_POSTSUBSCRIPT italic\_a italic\_r italic\_g start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT : per , italic\_a italic\_r italic\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT : book end\_POSTSUBSCRIPT ( bold\_c start\_POSTSUBSCRIPT italic\_S italic\_h italic\_a italic\_k italic\_e italic\_s italic\_p italic\_a italic\_r italic\_e end\_POSTSUBSCRIPT , bold\_c start\_POSTSUBSCRIPT italic\_M italic\_a italic\_c italic\_b italic\_e italic\_t italic\_h end\_POSTSUBSCRIPT )

(6)

Our observation is that the written order here is an artifact of the fact, traditionally, first-order logic was done by writing on a page, and that for computational purposes, the written order of the arguments is irrelevant, given their argument labels. Instead, we use a key-valued calculus formalism like:

Report issue for preceding element

(w⁢r⁢o⁢t⁢e,{a⁢r⁢g0:𝐜S⁢h⁢a⁢k⁢e⁢s⁢p⁢a⁢r⁢e,a⁢r⁢g1:𝐜M⁢a⁢c⁢b⁢e⁢t⁢h})𝑤𝑟𝑜𝑡𝑒conditional-set𝑎𝑟subscript𝑔0:subscript𝐜𝑆ℎ𝑎𝑘𝑒𝑠𝑝𝑎𝑟𝑒𝑎𝑟subscript𝑔1subscript𝐜𝑀𝑎𝑐𝑏𝑒𝑡ℎ(wrote,\\left\\{arg\_{0}:{\\bf c}\_{Shakespare},arg\_{1}:{\\bf c}\_{Macbeth}\\right\\})( italic\_w italic\_r italic\_o italic\_t italic\_e , { italic\_a italic\_r italic\_g start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT : bold\_c start\_POSTSUBSCRIPT italic\_S italic\_h italic\_a italic\_k italic\_e italic\_s italic\_p italic\_a italic\_r italic\_e end\_POSTSUBSCRIPT , italic\_a italic\_r italic\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT : bold\_c start\_POSTSUBSCRIPT italic\_M italic\_a italic\_c italic\_b italic\_e italic\_t italic\_h end\_POSTSUBSCRIPT } )

(7)

That is, it is easier to ignore the order of the arguments, and use a key-value map to index the arguments. In practice, as we will see, it is easier to encode implications if we ignore the order, and only use the function name and the labeled key-value pairs. Also, this formulation matches the way that an attention node works, in that an attention function can be described as mapping a query and a set of key-value pairs to an output \[[Vaswani et al., 2017](https://arxiv.org/html/2402.06557v1#bib.bibx49)\]. In the attention network, these objects are all vectors, while here they are symbols. To be clear, we are not claiming there is no book-keeping to do to get from surface structure to logical structure. We can associate each of the labeled dependencies each with a function application fom categorial grammar \[[Bar-Hillel, 1953](https://arxiv.org/html/2402.06557v1#bib.bibx2)\]. However, the pipeline can be greatly simplified on the parsing side and also on the knowledge representation side if we feel free to invent more flexible logical calculi (e.g., key-valued), so long as we prove consistency, completeness and sufficiency.

Report issue for preceding element

### 4.2 Language Definition

Report issue for preceding element

##### A Key-Value Calculus

Report issue for preceding element

Assume we have access to a labeled dependency parse as in Figure [1](https://arxiv.org/html/2402.06557v1#S4.F1 "Figure 1 ‣ A Key-Value Calculus ‣ 4.2 Language Definition ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

{dependency}

\[theme = simple\] {deptext}\[column sep=1em\] John & sent & a & letter & to & Sally  
\\deproot2ROOT \\depedge21subj \\depedge43det \\depedge24dobj \\depedge65case \\depedge26iobj

Report issue for preceding element

Figure 1: A labeled dependency parse. Without labels, we could not do semantics, so this is the most simple structure that can support semantics.

Report issue for preceding element

From this parse we can through some syntactic analysis extract a proposition of the rough form:

Report issue for preceding element

(send,{subj:John,dobj:a letter,iobj:Sally})sendmissing-subexpression:subjJohnmissing-subexpression:dobja lettermissing-subexpression:iobjSally(\\textsc{send},\\left\\{\\begin{aligned} &\\textsc{subj}:\\text{John},\\\\ &\\textsc{dobj}:\\text{a letter},\\\\ &\\textsc{iobj}:\\text{Sally}\\end{aligned}\\right\\})( send , { start\_ROW start\_CELL end\_CELL start\_CELL subj : John , end\_CELL end\_ROW start\_ROW start\_CELL end\_CELL start\_CELL dobj : a letter , end\_CELL end\_ROW start\_ROW start\_CELL end\_CELL start\_CELL iobj : Sally end\_CELL end\_ROW } )

(8)

By defining a predicate as close to the bare dependency structure as possible, we obviate the need to manage the book-keeping to enforce an arbitrary linear order on the arguments as in:

Report issue for preceding element

s⁢e⁢n⁢ds⁢u⁢b⁢j,d⁢o⁢b⁢j,i⁢o⁢b⁢j⁢(John,a letter,Sally)𝑠𝑒𝑛subscript𝑑𝑠𝑢𝑏𝑗𝑑𝑜𝑏𝑗𝑖𝑜𝑏𝑗Johna letterSallysend\_{subj,dobj,iobj}(\\text{John},\\text{a letter},\\text{Sally})italic\_s italic\_e italic\_n italic\_d start\_POSTSUBSCRIPT italic\_s italic\_u italic\_b italic\_j , italic\_d italic\_o italic\_b italic\_j , italic\_i italic\_o italic\_b italic\_j end\_POSTSUBSCRIPT ( John , a letter , Sally )

(9)

We still use a first-order style sometimes in the text to save space where the intended key-value translation is hopefully clear.

Report issue for preceding element

##### Truth Values

Report issue for preceding element

There are two boolean truth values, true, which we write as 1111 and false, which we write as 00. The nodes of primary interest in queries to our graphical model are about the values of propositions, usually denoted 𝐩𝐩{\\bf p}bold\_p. We can query the probabilities P⁢(𝐩\=1)𝑃𝐩1P({\\bf p}=1)italic\_P ( bold\_p = 1 ) and P⁢(𝐩\=0)𝑃𝐩0P({\\bf p}=0)italic\_P ( bold\_p = 0 ). That is, we assume that each proposition is either definitely true or false, and we do not know which, but we can assign a probability in \[0,1\]01\[0,1\]\[ 0 , 1 \].

Report issue for preceding element

##### Entities

Report issue for preceding element

An entity is identified by a string e𝑒eitalic\_e and corresponds to an object in our information retrieval database, e.g., Taylor Swift, Beyonce, USA, China.

Report issue for preceding element

##### Types

Report issue for preceding element

A type τ𝜏\\tauitalic\_τ is identified by a string. We will assume that each entity exhibits a non-negative number of types. In information retrieval some relevant types are business, individual, group, book, or product. Usually the type is clear from context and we will usually not write τ𝜏\\tauitalic\_τ.

Report issue for preceding element

##### Constants

Report issue for preceding element

A constant (or constant reference) is a pair (e,τ)𝑒𝜏(e,\\tau)( italic\_e , italic\_τ ) of entity identifier and type. The constant refers to a specific entity. For example, the entity usa exhibits the type country, so its constant reference would be:

Report issue for preceding element

𝐜u⁢s⁢a\=𝐜𝐨𝐧𝐬𝐭𝐚𝐧𝐭⁢(usa,country)subscript𝐜𝑢𝑠𝑎𝐜𝐨𝐧𝐬𝐭𝐚𝐧𝐭usacountry{\\bf c}\_{usa}=\\textbf{constant}(\\textsc{usa},\\textsc{country})bold\_c start\_POSTSUBSCRIPT italic\_u italic\_s italic\_a end\_POSTSUBSCRIPT = constant ( usa , country )

(10)

##### Variables

Report issue for preceding element

A variable is defined by a type τ𝜏\\tauitalic\_τ.

Report issue for preceding element

𝐱c⁢o⁢u⁢n⁢t⁢r⁢y\=𝐯𝐚𝐫𝐢𝐚𝐛𝐥𝐞⁢(country)subscript𝐱𝑐𝑜𝑢𝑛𝑡𝑟𝑦𝐯𝐚𝐫𝐢𝐚𝐛𝐥𝐞country{\\bf x}\_{country}=\\textbf{variable}(\\textsc{country})bold\_x start\_POSTSUBSCRIPT italic\_c italic\_o italic\_u italic\_n italic\_t italic\_r italic\_y end\_POSTSUBSCRIPT = variable ( country )

(11)

A variable can be instantiated by any constant of the same type.

Report issue for preceding element

##### Function Names

Report issue for preceding element

A function name f𝑓fitalic\_f is a string, e.g., like or date.

Report issue for preceding element

##### Arguments

Report issue for preceding element

An argument aτsubscript𝑎𝜏a\_{\\tau}italic\_a start\_POSTSUBSCRIPT italic\_τ end\_POSTSUBSCRIPT is an object that wraps either a constant 𝐜τsubscript𝐜𝜏{\\bf c}\_{\\tau}bold\_c start\_POSTSUBSCRIPT italic\_τ end\_POSTSUBSCRIPT or a variable 𝐱τsubscript𝐱𝜏{\\bf x}\_{\\tau}bold\_x start\_POSTSUBSCRIPT italic\_τ end\_POSTSUBSCRIPT. Given an argument, we can tell which type of object it wraps (𝐜τsubscript𝐜𝜏{\\bf c}\_{\\tau}bold\_c start\_POSTSUBSCRIPT italic\_τ end\_POSTSUBSCRIPT or 𝐱τsubscript𝐱𝜏{\\bf x}\_{\\tau}bold\_x start\_POSTSUBSCRIPT italic\_τ end\_POSTSUBSCRIPT), and also recover the wrapped object.

Report issue for preceding element

##### Role Labels

Report issue for preceding element

Each role label r𝑟ritalic\_r is a string from a bounded set, e.g. subj, dobj or iobj. The role label indexes the argument position that an argument plays for a function. A labeled argument is a pair (r,a)𝑟𝑎(r,a)( italic\_r , italic\_a ) of role and argument.

Report issue for preceding element

##### Role Sets and Maps

Report issue for preceding element

A set of roles 𝐫\={r}r∈𝐫𝐫subscript𝑟𝑟𝐫{\\bf r}=\\left\\{r\\right\\}\_{r\\in{\\bf r}}bold\_r = { italic\_r } start\_POSTSUBSCRIPT italic\_r ∈ bold\_r end\_POSTSUBSCRIPT is called a role set. A role-argument mapping is a map 𝐦𝐫\={(r,a)}r∈𝐫subscript𝐦𝐫subscript𝑟𝑎𝑟𝐫{\\bf m}\_{\\bf r}=\\left\\{(r,a)\\right\\}\_{r\\in{\\bf r}}bold\_m start\_POSTSUBSCRIPT bold\_r end\_POSTSUBSCRIPT = { ( italic\_r , italic\_a ) } start\_POSTSUBSCRIPT italic\_r ∈ bold\_r end\_POSTSUBSCRIPT with role set 𝐫𝐫{\\bf r}bold\_r. The open roles in 𝐦𝐦{\\bf m}bold\_m are those pair (r,a)𝑟𝑎(r,a)( italic\_r , italic\_a ) where a𝑎aitalic\_a wraps a variable. The filled roles are those where a𝑎aitalic\_a wraps a constant.

Report issue for preceding element

##### Predicates

Report issue for preceding element

A predicate’s type is defined by pair of a function name and a set of roles labels.

Report issue for preceding element

τ⁢(𝐪)\=(f,𝐫)𝜏𝐪𝑓𝐫\\tau({\\bf q})=(f,{\\bf r})italic\_τ ( bold\_q ) = ( italic\_f , bold\_r )

(12)

A predicate instance is a pair of a function name and a role-argument mapping:

Report issue for preceding element

𝐪\=(f,𝐦𝐫)𝐪𝑓subscript𝐦𝐫{\\bf q}=(f,{\\bf m}\_{\\bf r})bold\_q = ( italic\_f , bold\_m start\_POSTSUBSCRIPT bold\_r end\_POSTSUBSCRIPT )

(13)

An example of a predicate is:

Report issue for preceding element

𝐪\=(like,{sub:𝐱j⁢a⁢c⁢k,obj:𝐱j⁢i⁢l⁢l})𝐪likeconditional-setsub:subscript𝐱𝑗𝑎𝑐𝑘objsubscript𝐱𝑗𝑖𝑙𝑙{\\bf q}=(\\textsc{like},\\left\\{\\textsc{sub}:{\\bf x}\_{jack},\\textsc{obj}:{\\bf x}% \_{jill}\\right\\})bold\_q = ( like , { sub : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , obj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT } )

(14)

𝐪𝐪{\\bf q}bold\_q does not have a truth value, and we cannot ask P⁢(𝐪\=1)𝑃𝐪1P({\\bf q}=1)italic\_P ( bold\_q = 1 ), because of the presence of open roles and so unbound variables 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT and 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT. Only when these variables are replaced by constants, referring to specific entities, will we have a truth value to estimate a probability for.

Report issue for preceding element

##### Propositions

Report issue for preceding element

A predicate with zero open roles is called a proposition, usually denoted 𝐩𝐩{\\bf p}bold\_p, e.g.

Report issue for preceding element

𝐩\=(like,{sub:𝐜j⁢a⁢c⁢k⁢1,obj:𝐜j⁢i⁢l⁢l⁢1})𝐩likeconditional-setsub:subscript𝐜𝑗𝑎𝑐𝑘1objsubscript𝐜𝑗𝑖𝑙𝑙1{\\bf p}=(\\textsc{like},\\left\\{\\textsc{sub}:{\\bf c}\_{jack1},\\textsc{obj}:{\\bf c% }\_{jill1}\\right\\})bold\_p = ( like , { sub : bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k 1 end\_POSTSUBSCRIPT , obj : bold\_c start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l 1 end\_POSTSUBSCRIPT } )

(15)

Having no open roles, a proposition is fully grounded and so has a probability, and we can ask P⁢(𝐩\=1)𝑃𝐩1P({\\bf p}=1)italic\_P ( bold\_p = 1 ). E.g., in this case, we can ask whether l⁢i⁢k⁢e⁢(𝐜j⁢a⁢c⁢k⁢1,𝐜j⁢i⁢l⁢l⁢1)𝑙𝑖𝑘𝑒subscript𝐜𝑗𝑎𝑐𝑘1subscript𝐜𝑗𝑖𝑙𝑙1like({\\bf c}\_{jack1},{\\bf c}\_{jill1})italic\_l italic\_i italic\_k italic\_e ( bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k 1 end\_POSTSUBSCRIPT , bold\_c start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l 1 end\_POSTSUBSCRIPT ) in particular.

Report issue for preceding element

### 4.3 Quantification and Implication

Report issue for preceding element

#### 4.3.1 Statistical Inference

Report issue for preceding element

In the traditional first-order calculus ∀x,A⁢(x)→B⁢(x)→for-all𝑥𝐴𝑥𝐵𝑥\\forall x,A(x)\\rightarrow B(x)∀ italic\_x , italic\_A ( italic\_x ) → italic\_B ( italic\_x ) means that B𝐵Bitalic\_B always follows A𝐴Aitalic\_A. We want to generalize ∀for-all\\forall∀ with a statistical notion Ψ⁢x,A⁢(x)→B⁢(x)→Ψ𝑥𝐴𝑥𝐵𝑥\\Psi x,A(x)\\rightarrow B(x)roman\_Ψ italic\_x , italic\_A ( italic\_x ) → italic\_B ( italic\_x ), which means, more generally, that B𝐵Bitalic\_B follows A𝐴Aitalic\_A with some probability. Then, we have the option to estimate ΨΨ\\Psiroman\_Ψ from data.

Report issue for preceding element

#### 4.3.2 Predicate Implication Links

Report issue for preceding element

##### Example

Report issue for preceding element

We will introduce the running example of binary dating, in which we have a bipartite graph with two types of entities, those of type 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT and those of type 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT, and we have a predicate of interest:

Report issue for preceding element

(date,{subj:𝐱j⁢a⁢c⁢k,dobj:𝐱j⁢i⁢l⁢l})dateconditional-setsubj:subscript𝐱𝑗𝑎𝑐𝑘dobjsubscript𝐱𝑗𝑖𝑙𝑙(\\textsc{date},\\left\\{\\textsc{subj}:{\\bf x}\_{jack},\\textsc{dobj}:{\\bf x}\_{jill% }\\right\\})( date , { subj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , dobj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT } )

(16)

This returns true if 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT is dating 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT. Now if 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT likes 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT, they are more likely to date. We can represent this in our key-value calculus as:

Report issue for preceding element

Ψ⁢\[𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l\]⁢(like,{subj:𝐱j⁢a⁢c⁢k,dobj:𝐱j⁢i⁢l⁢l,})→(date⁢{subj:𝐱j⁢a⁢c⁢k,dobj:𝐱j⁢i⁢l⁢l,})→Ψsubscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙likemissing-subexpression:subjsubscript𝐱𝑗𝑎𝑐𝑘missing-subexpression:dobjsubscript𝐱𝑗𝑖𝑙𝑙datemissing-subexpression:subjsubscript𝐱𝑗𝑎𝑐𝑘missing-subexpression:dobjsubscript𝐱𝑗𝑖𝑙𝑙\\Psi\[{\\bf x}\_{jack},{\\bf x}\_{jill}\]\\left(\\textsc{like},\\left\\{\\begin{aligned} % &\\textsc{subj}:{\\bf x}\_{jack},\\\\ &\\textsc{dobj}:{\\bf x}\_{jill},\\\\ \\end{aligned}\\right\\}\\right)\\rightarrow\\left(\\textsc{date}\\left\\{\\begin{% aligned} &\\textsc{subj}:{\\bf x}\_{jack},\\\\ &\\textsc{dobj}:{\\bf x}\_{jill},\\\\ \\end{aligned}\\right\\}\\right)roman\_Ψ \[ bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT \] ( like , { start\_ROW start\_CELL end\_CELL start\_CELL subj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , end\_CELL end\_ROW start\_ROW start\_CELL end\_CELL start\_CELL dobj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , end\_CELL end\_ROW } ) → ( date { start\_ROW start\_CELL end\_CELL start\_CELL subj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , end\_CELL end\_ROW start\_ROW start\_CELL end\_CELL start\_CELL dobj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , end\_CELL end\_ROW } )

(17)

We can also represent the related link that 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT and 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT are more likely to date if 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT likes 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT:

Report issue for preceding element

Ψ⁢\[𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l\]⁢(like,{subj:𝐱j⁢i⁢l⁢l,dobj:𝐱j⁢a⁢c⁢k,})→(date⁢{subj:𝐱j⁢a⁢c⁢k,dobj:𝐱j⁢i⁢l⁢l,})→Ψsubscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙likemissing-subexpression:subjsubscript𝐱𝑗𝑖𝑙𝑙missing-subexpression:dobjsubscript𝐱𝑗𝑎𝑐𝑘datemissing-subexpression:subjsubscript𝐱𝑗𝑎𝑐𝑘missing-subexpression:dobjsubscript𝐱𝑗𝑖𝑙𝑙\\Psi\[{\\bf x}\_{jack},{\\bf x}\_{jill}\]\\left(\\textsc{like},\\left\\{\\begin{aligned} % &\\textsc{subj}:{\\bf x}\_{jill},\\\\ &\\textsc{dobj}:{\\bf x}\_{jack},\\\\ \\end{aligned}\\right\\}\\right)\\rightarrow\\left(\\textsc{date}\\left\\{\\begin{% aligned} &\\textsc{subj}:{\\bf x}\_{jack},\\\\ &\\textsc{dobj}:{\\bf x}\_{jill},\\\\ \\end{aligned}\\right\\}\\right)roman\_Ψ \[ bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT \] ( like , { start\_ROW start\_CELL end\_CELL start\_CELL subj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , end\_CELL end\_ROW start\_ROW start\_CELL end\_CELL start\_CELL dobj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , end\_CELL end\_ROW } ) → ( date { start\_ROW start\_CELL end\_CELL start\_CELL subj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , end\_CELL end\_ROW start\_ROW start\_CELL end\_CELL start\_CELL dobj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , end\_CELL end\_ROW } )

(18)

##### Role Set Mapping

Report issue for preceding element

Comparing [17](https://arxiv.org/html/2402.06557v1#S4.E17 "17 ‣ Example ‣ 4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") to [18](https://arxiv.org/html/2402.06557v1#S4.E18 "18 ‣ Example ‣ 4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), we see that [17](https://arxiv.org/html/2402.06557v1#S4.E17 "17 ‣ Example ‣ 4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") maintains the same role-argument assignments in premise as conclusion:

Report issue for preceding element

{subj:subj,dobj:dobj}conditional-setsubj:subjdobjdobj\\left\\{\\textsc{subj}:\\textsc{subj},\\textsc{dobj}:\\textsc{dobj}\\right\\}{ subj : subj , dobj : dobj }

(19)

In [18](https://arxiv.org/html/2402.06557v1#S4.E18 "18 ‣ Example ‣ 4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), the roles are reversed:

Report issue for preceding element

{subj:dobj,dobj:subj}conditional-setsubj:dobjdobjsubj\\left\\{\\textsc{subj}:\\textsc{dobj},\\textsc{dobj}:\\textsc{subj}\\right\\}{ subj : dobj , dobj : subj }

(20)

In order to allow both possibilities, between any conclusion 𝐪csubscript𝐪𝑐{\\bf q}\_{c}bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT and premise 𝐪asubscript𝐪𝑎{\\bf q}\_{a}bold\_q start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT, we introduce the role set mapping, which is a map {r,s}𝑟𝑠\\left\\{r,s\\right\\}{ italic\_r , italic\_s }, where each entry (r,s)𝑟𝑠(r,s)( italic\_r , italic\_s ) indicates that the argument for role r𝑟ritalic\_r in 𝐪asubscript𝐪𝑎{\\bf q}\_{a}bold\_q start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT should be used to fill role s𝑠sitalic\_s in 𝐪csubscript𝐪𝑐{\\bf q}\_{c}bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT.

Report issue for preceding element

##### Predicate Implication Link

Report issue for preceding element

A single predicate implication link is a triple:

Report issue for preceding element

Ψ⁢(𝐪a,𝐪c,{r,s})Ψsubscript𝐪𝑎subscript𝐪𝑐𝑟𝑠\\Psi({\\bf q}\_{a},{\\bf q}\_{c},\\left\\{r,s\\right\\})roman\_Ψ ( bold\_q start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT , bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT , { italic\_r , italic\_s } )

(21)

where 𝐪asubscript𝐪𝑎{\\bf q}\_{a}bold\_q start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT and 𝐪csubscript𝐪𝑐{\\bf q}\_{c}bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT are predicates, and where {r,s}𝑟𝑠\\left\\{r,s\\right\\}{ italic\_r , italic\_s } is an appropriate role mapping between the two. In our current implementation, we require that all open roles in each of 𝐪asubscript𝐪𝑎{\\bf q}\_{a}bold\_q start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT and 𝐪csubscript𝐪𝑐{\\bf q}\_{c}bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT be filled, and that 𝐪asubscript𝐪𝑎{\\bf q}\_{a}bold\_q start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT have less than or equal to the number of open roles of 𝐪csubscript𝐪𝑐{\\bf q}\_{c}bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT.

Report issue for preceding element

#### 4.3.3 Conjoined Predicate Implication

Report issue for preceding element

##### Motivation

Report issue for preceding element

At a high level, the implication links correspond to patterns of features that we can train and reuse over proposition factors. Suppose we want to use a linear model for these features, either because it is interpretable or because it is faster. The problem with linear models, in general, is that they cannot separate all functions. For example, xor cannot be separated, if the problem is interpreted naively \[[Minsky and Papert, 1969](https://arxiv.org/html/2402.06557v1#bib.bibx26)\]. However, e.g., xor can be separated if we are allowed to conjoin (or combine, or take a boolean combination of) the input features. In the case of dating, 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT and 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT will in a modern context only date if they both like each other. To represent this, we want a feature that only fires if both l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙like({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) and l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ), i.e.:

Report issue for preceding element

Ψ⁢\[𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l\]⁢\[{l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)∧l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)}→d⁢a⁢t⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)\]Ψsubscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙delimited-\[\]→𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘𝑑𝑎𝑡𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙\\Psi\[{\\bf x}\_{jack},{\\bf x}\_{jill}\]\\left\[\\left\\{like({\\bf x}\_{jack},{\\bf x}\_{% jill})\\wedge like({\\bf x}\_{jill},{\\bf x}\_{jack})\\right\\}\\rightarrow date({\\bf x% }\_{jack},{\\bf x}\_{jill})\\right\]roman\_Ψ \[ bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT \] \[ { italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) ∧ italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) } → italic\_d italic\_a italic\_t italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) \]

(22)

We discuss the role of conjunction in producing higher-level features in Section [5.5](https://arxiv.org/html/2402.06557v1#S5.SS5 "5.5 Conjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

##### Formulation

Report issue for preceding element

Where 𝐪aisubscript𝐪subscript𝑎𝑖{\\bf q}\_{a\_{i}}bold\_q start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT are each predicates, we use 𝐡𝐡{\\bf h}bold\_h as short-hand for a group (ordered list) of predicates:

Report issue for preceding element

𝐡a\=\[𝐪a1,…,𝐪an\]subscript𝐡𝑎subscript𝐪subscript𝑎1…subscript𝐪subscript𝑎𝑛{\\bf h}\_{a}=\[{\\bf q}\_{a\_{1}},...,{\\bf q}\_{a\_{n}}\]bold\_h start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT = \[ bold\_q start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT , … , bold\_q start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT \]

Where 𝐪csubscript𝐪𝑐{\\bf q}\_{c}bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT is a conclusion predicate define the conjoined implication Ψ⁢(𝐡a,𝐪c)Ψsubscript𝐡𝑎subscript𝐪𝑐\\Psi({\\bf h}\_{a},{\\bf q}\_{c})roman\_Ψ ( bold\_h start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT , bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT ) as:

Report issue for preceding element

Ψ⁢(𝐡a,𝐪c)\=\[(𝐡a1,𝐪c,{r,s}a1)∧…∧(𝐡n,𝐪c,{r,s}an)\]Ψsubscript𝐡𝑎subscript𝐪𝑐delimited-\[\]subscript𝐡subscript𝑎1subscript𝐪𝑐subscript𝑟𝑠subscript𝑎1…subscript𝐡𝑛subscript𝐪𝑐subscript𝑟𝑠subscript𝑎𝑛\\Psi({\\bf h}\_{a},{\\bf q}\_{c})=\\left\[({\\bf h}\_{a\_{1}},{\\bf q}\_{c},\\left\\{r,s% \\right\\}\_{a\_{1}})\\wedge...\\wedge({\\bf h}\_{n},{\\bf q}\_{c},\\left\\{r,s\\right\\}\_{a% \_{n}})\\right\]roman\_Ψ ( bold\_h start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT , bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT ) = \[ ( bold\_h start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT , bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT , { italic\_r , italic\_s } start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ) ∧ … ∧ ( bold\_h start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT , bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT , { italic\_r , italic\_s } start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ) \]

(23)

Here, we assume that each {r,s}aisubscript𝑟𝑠subscript𝑎𝑖\\left\\{r,s\\right\\}\_{a\_{i}}{ italic\_r , italic\_s } start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT is appropriate to match the open roles of 𝐡aisubscript𝐡subscript𝑎𝑖{\\bf h}\_{a\_{i}}bold\_h start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT to 𝐪csubscript𝐪𝑐{\\bf q}\_{c}bold\_q start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT. The form [23](https://arxiv.org/html/2402.06557v1#S4.E23 "23 ‣ Formulation ‣ 4.3.3 Conjoined Predicate Implication ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") allows us to state an inferential link like [22](https://arxiv.org/html/2402.06557v1#S4.E22 "22 ‣ Motivation ‣ 4.3.3 Conjoined Predicate Implication ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

5 The Proposition Graph
-----------------------

Report issue for preceding element

### 5.1 Markov Assumption

Report issue for preceding element

The essential feature of a graphical model is that it makes a Markov assumption, in which each variable in the graph is independent of all nodes, given the values of its neighbors. Because the edges are directed, the neighbors of a node are its parents and its childen. In our Bayesian Network, each factor in the graph has the form:

Report issue for preceding element

Ψ⁢(𝐳|𝐳a1,…,𝐳an)Ψconditional𝐳subscript𝐳subscript𝑎1…subscript𝐳subscript𝑎𝑛\\Psi({\\bf z}\\ |\\ {\\bf z}\_{a\_{1}},...,{\\bf z}\_{a\_{n}})roman\_Ψ ( bold\_z | bold\_z start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT , … , bold\_z start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT )

(24)

In this case, we would say that 𝐳𝐳{\\bf z}bold\_z is the child of each 𝐳aisubscript𝐳subscript𝑎𝑖{\\bf z}\_{a\_{i}}bold\_z start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT and each 𝐳aisubscript𝐳subscript𝑎𝑖{\\bf z}\_{a\_{i}}bold\_z start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT is a parent of 𝐳𝐳{\\bf z}bold\_z. Conversely, 𝐳𝐳{\\bf z}bold\_z can also have effects on its children as in:

Report issue for preceding element

Ψ⁢(𝐳c|𝐳,𝐳b1,…,𝐳bn−1)Ψconditionalsubscript𝐳𝑐𝐳subscript𝐳subscript𝑏1…subscript𝐳subscript𝑏𝑛1\\Psi({\\bf z}\_{c}\\ |\\ {\\bf z},{\\bf z}\_{b\_{1}},...,{\\bf z}\_{b\_{n-1}})roman\_Ψ ( bold\_z start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT | bold\_z , bold\_z start\_POSTSUBSCRIPT italic\_b start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT , … , bold\_z start\_POSTSUBSCRIPT italic\_b start\_POSTSUBSCRIPT italic\_n - 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT )

(25)

Here, the 𝐳bisubscript𝐳subscript𝑏𝑖{\\bf z}\_{b\_{i}}bold\_z start\_POSTSUBSCRIPT italic\_b start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT are other parents of 𝐳csubscript𝐳𝑐{\\bf z}\_{c}bold\_z start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT. The Markov assumption says that we can know everything we need to know about 𝐳𝐳{\\bf z}bold\_z if we know the values of its parents and its children, i.e., 𝐳𝐳{\\bf z}bold\_z is independent of all other nodes in the network, given its neighbors.

Report issue for preceding element

### 5.2 Lazy Graph Storage

Report issue for preceding element

##### The Full Graph is Unbounded

Report issue for preceding element

For an unbounded set of entities, there are an unbounded number of possible propositions 𝐩𝐩{\\bf p}bold\_p, many of which will never be relevant. For example, consider the predicate of is President of the United States. This only applies in practice to one person, but could, in principle, apply to billions. Thus, storing all possible propositions in hard disk memory would not be possible.

Report issue for preceding element

##### Stored vs. Dynamically Calculated Probabilities

Report issue for preceding element

We have two options in the system for estimating the probability of a proposition 𝐩𝐩{\\bf p}bold\_p. The first is that the probability for the proposition is alredy computed before the query is issued. In the example of 𝐱p⁢e⁢r⁢s⁢o⁢nsubscript𝐱𝑝𝑒𝑟𝑠𝑜𝑛{\\bf x}\_{person}bold\_x start\_POSTSUBSCRIPT italic\_p italic\_e italic\_r italic\_s italic\_o italic\_n end\_POSTSUBSCRIPT is the President of the United States, this can be set to true in long-term storage for the unique individual who occupies this slot. For anyone else, we can use generic reasoning, like there is only one President, and right now the President is someone else, etc., to answer no.

Report issue for preceding element

##### Use of the Markov Assumption

Report issue for preceding element

During training of Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT, we only train local factors assuming fully observed data, which does not require a full proposition graph to be created, but only the relevant factors (see Section [6](https://arxiv.org/html/2402.06557v1#S6 "6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")) to be identified. During inference, we create the proposition graph dynamically at run time from the implication graph, described in Section [6](https://arxiv.org/html/2402.06557v1#S6 "6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."). Because of the Markov Assumption, and the mechanics of universal quantification, for any proposition 𝐩𝐩{\\bf p}bold\_p, we can determine exactly which other propositions are relevant to determining 𝐩𝐩{\\bf p}bold\_p.

Report issue for preceding element

### 5.3 Boolean Algebra

Report issue for preceding element

For reasons of logical completeness, and also computational efficiency, we split the graph into two kinds of junctions, or factor types:

Report issue for preceding element

1.  1.
    
    conjunction factors, denoted Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT
    
    Report issue for preceding element
    
2.  2.
    
    disjunction factors, denoted Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT.
    
    Report issue for preceding element
    

The computation in the graph alternates between these: a conjunction factor Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT feeds into a disjunction factor Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT, and vice versa, as depicted in Figure [2](https://arxiv.org/html/2402.06557v1#S5.F2 "Figure 2 ‣ 5.3 Boolean Algebra ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT\[boy lonely\]Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT\[girl exciting\]Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPTΨ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPTΨ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT\[boy likes girl\]Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT\[girl likes boy\]Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPTΨ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT\[boy dates girl\]Report issue for preceding element

Figure 2: A boolean network that alternates between and and or gates.

Report issue for preceding element

### 5.4 Bipartite Graph

Report issue for preceding element

Because the factor types Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT and Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT always alternate, we have a bipartite graph. Suppose 𝐩1,…,𝐩nsubscript𝐩1…subscript𝐩𝑛{\\bf p}\_{1},...,{\\bf p}\_{n}bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_p start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT are each propositions. Then we say

Report issue for preceding element

𝐠\={𝐩1∧…∧𝐩n}𝐠subscript𝐩1…subscript𝐩𝑛{\\bf g}=\\left\\{{\\bf p}\_{1}\\wedge...\\wedge{\\bf p}\_{n}\\right\\}bold\_g = { bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT ∧ … ∧ bold\_p start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT }

(26)

is a proposition group, which are interpreted as conjoined. Then, the two types of variables in the graph then are:

Report issue for preceding element

1.  1.
    
    𝐩𝐩{\\bf p}bold\_p, which represents a single proposition
    
    Report issue for preceding element
    
2.  2.
    
    𝐠𝐠{\\bf g}bold\_g, which represents a conjoined proposition group
    
    Report issue for preceding element
    

For many purposes in the graphical model (e.g., message passing calculuations), we can abstract over whether a node is 𝐠𝐠{\\bf g}bold\_g and 𝐩𝐩{\\bf p}bold\_p, and we refer to generic graphical nodes as 𝐳𝐳{\\bf z}bold\_z. It is to be understood that each 𝐳𝐳{\\bf z}bold\_z actually wraps a 𝐠𝐠{\\bf g}bold\_g or a 𝐩𝐩{\\bf p}bold\_p, and that we can get either the underlying type or underlying value from any 𝐳𝐳{\\bf z}bold\_z at any time.

Report issue for preceding element

### 5.5 Conjunction Nodes

Report issue for preceding element

#### 5.5.1 Deterministic Definition

Report issue for preceding element

The conjunctive factor Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT is defined in terms of the and gate:

Report issue for preceding element

𝒂𝒏𝒅⁢(𝐩1,…,𝐩n)\=𝐩1∧…∧𝐩n𝒂𝒏𝒅subscript𝐩1…subscript𝐩𝑛subscript𝐩1…subscript𝐩𝑛\\textbf{\\em and}({\\bf p}\_{1},...,{\\bf p}\_{n})={\\bf p}\_{1}\\wedge...\\wedge{\\bf p% }\_{n}and ( bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_p start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) = bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT ∧ … ∧ bold\_p start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT

(27)

Then:

Report issue for preceding element

Ψ𝒂𝒏𝒅⁢(𝐠|𝐩1,…,𝐩n)\={1if 𝐠\=\=𝒂𝒏𝒅(𝐩1,…,𝐩n),0otherwise\\Psi\_{\\textbf{\\em and}}({\\bf g}\\ |\\ {\\bf p}\_{1},\\ldots,{\\bf p}\_{n})=\\begin{% cases}1&\\text{if }{\\bf g}==\\textbf{\\em and}({\\bf p}\_{1},\\ldots,{\\bf p}\_{n}),\\\\ 0&\\text{otherwise}\\end{cases}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT ( bold\_g | bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_p start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) = { start\_ROW start\_CELL 1 end\_CELL start\_CELL if bold\_g = = and ( bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_p start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) , end\_CELL end\_ROW start\_ROW start\_CELL 0 end\_CELL start\_CELL otherwise end\_CELL end\_ROW

(28)

This is used in the completeness proof, but also in learned models.

Report issue for preceding element

#### 5.5.2 Higher-Level Features

Report issue for preceding element

Let us meditate on the fact that the Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT factor is always deterministic, i.e., we do not train this even when we are interested in statistical inference. One way to justify this is that, intuitively, and’s role is to create higher-level features, between which we can learn relationships. This is like a discrete analog to the higher-level features that multi-layer networks learn \[[Rumelhart et al., 1986](https://arxiv.org/html/2402.06557v1#bib.bibx39), [LeCun et al., 1989](https://arxiv.org/html/2402.06557v1#bib.bibx22)\]. For example, suppose we are given the information about l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙like({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) and l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) as simple features to predict d⁢a⁢t⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑑𝑎𝑡𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙date({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_d italic\_a italic\_t italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ). Supposing the relevant higher level feature is l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)∧l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jack},{\\bf x}\_{jill})\\land like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) ∧ italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ), one option is run these features through a multi-layer network, which will be able to learn this feature, in a differentiable way. But, this higher-level feature emerges as an effectively emergent behavior. This leads to the problem of interpretability \[[Hinton et al., 2015](https://arxiv.org/html/2402.06557v1#bib.bibx18), [Ribeiro et al., 2016](https://arxiv.org/html/2402.06557v1#bib.bibx36), [Lundberg and Lee, 2017](https://arxiv.org/html/2402.06557v1#bib.bibx24)\]. The QBBN is another interpretation of interpretability, because the features must be explicitly conjoined in order to work. The problem is not one of interpreting the model, but constructing the model in the first place, since the individual function names and role labels underlying logical “language” are latent \[[Steedman, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx42)\], and presumably would be discovered through something analogous to category splitting in a generative model \[[Petrov et al., 2006](https://arxiv.org/html/2402.06557v1#bib.bibx33)\].

Report issue for preceding element

### 5.6 Disjunction Nodes

Report issue for preceding element

#### 5.6.1 Deterministic Definition

Report issue for preceding element

The deterministic disjunctive factor Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT used for the completeness proof (also see \[[Coppola, 2024a](https://arxiv.org/html/2402.06557v1#bib.bibx7)\]), is defined in terms of the or gate:

Report issue for preceding element

𝒐𝒓⁢(𝐠1,…,𝐠n)\=𝐠1∨…∨𝐠n𝒐𝒓subscript𝐠1…subscript𝐠𝑛subscript𝐠1…subscript𝐠𝑛\\textbf{\\em or}({\\bf g}\_{1},\\ldots,{\\bf g}\_{n})={\\bf g}\_{1}\\vee\\ldots\\vee{\\bf g% }\_{n}or ( bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) = bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT ∨ … ∨ bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT

(29)

The deterministic version of or, used in the completeness proof, and can be used any time we want exact logical or, is defined as:

Report issue for preceding element

Ψ𝒐𝒓⁢(𝐩|𝐠1,…,𝐠n)\={1if 𝐩\=\=𝒐𝒓(𝐠1,…,𝐠n),0otherwise\\Psi\_{\\textbf{\\em or}}({\\bf p}\\ |\\ {\\bf g}\_{1},\\ldots,{\\bf g}\_{n})=\\begin{% cases}1&\\text{if }{\\bf p}==\\textbf{\\em or}({\\bf g}\_{1},\\ldots,{\\bf g}\_{n}),\\\\ 0&\\text{otherwise}\\end{cases}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT ( bold\_p | bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) = { start\_ROW start\_CELL 1 end\_CELL start\_CELL if bold\_p = = or ( bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) , end\_CELL end\_ROW start\_ROW start\_CELL 0 end\_CELL start\_CELL otherwise end\_CELL end\_ROW

(30)

When interested in statistical inference, we learn this model, as discussed in Section [5.6.2](https://arxiv.org/html/2402.06557v1#S5.SS6.SSS2 "5.6.2 Learned Disjunctive Model ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

#### 5.6.2 Learned Disjunctive Model

Report issue for preceding element

For the learned model, we model Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT using linear exponential model. For a boolean variable 𝐩𝐩{\\bf p}bold\_p with boolean features 𝐠1,…,𝐠nsubscript𝐠1…subscript𝐠𝑛{\\bf g}\_{1},...,{\\bf g}\_{n}bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT, the factor potential has the form:

Report issue for preceding element

Ψ𝒐𝒓⁢(𝐩|𝐠1,…,𝐠n)\=exp⁡{∑i\=1n𝐰⋅ϕ⁢(𝐩,𝐠i)}subscriptΨ𝒐𝒓conditional𝐩subscript𝐠1…subscript𝐠𝑛superscriptsubscript𝑖1𝑛⋅𝐰italic-ϕ𝐩subscript𝐠𝑖\\Psi\_{\\textbf{\\em or}}({\\bf p}\\ |\\ {\\bf g}\_{1},...,{\\bf g}\_{n})=\\exp{\\left\\{% \\sum\_{i=1}^{n}{{\\bf w}\\cdot\\phi({\\bf p},{\\bf g}\_{i})}\\right\\}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT ( bold\_p | bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) = roman\_exp { ∑ start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT bold\_w ⋅ italic\_ϕ ( bold\_p , bold\_g start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ) }

(31)

Here, 𝐰𝐰{\\bf w}bold\_w is a weight vector, and ϕ⁢(𝐩,𝐠i)italic-ϕ𝐩subscript𝐠𝑖\\phi({\\bf p},{\\bf g}\_{i})italic\_ϕ ( bold\_p , bold\_g start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ) is a feature discussed in Section [6.6](https://arxiv.org/html/2402.06557v1#S6.SS6 "6.6 Feature Function ‣ 6 The Implication Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."). The probability P⁢(𝐩|𝐠1,…,𝐠n)𝑃conditional𝐩subscript𝐠1…subscript𝐠𝑛P({\\bf p}\\ |\\ {\\bf g}\_{1},...,{\\bf g}\_{n})italic\_P ( bold\_p | bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) is obtained by normalization over the two possible values for 𝐩∈{0,1}𝐩01{\\bf p}\\in\\left\\{0,1\\right\\}bold\_p ∈ { 0 , 1 }:

Report issue for preceding element

P⁢(𝐩\=p|𝐠1,…,𝐠n)\=Ψ𝒐𝒓⁢(p|𝐠1,…,𝐠n)Ψ𝒐𝒓⁢(1|𝐠1,…,𝐠n)+Ψ𝒐𝒓⁢(0|𝐠1,…,𝐠n)𝑃𝐩conditional𝑝subscript𝐠1…subscript𝐠𝑛subscriptΨ𝒐𝒓conditional𝑝subscript𝐠1…subscript𝐠𝑛subscriptΨ𝒐𝒓conditional1subscript𝐠1…subscript𝐠𝑛subscriptΨ𝒐𝒓conditional0subscript𝐠1…subscript𝐠𝑛P({\\bf p}=p\\ |\\ {\\bf g}\_{1},...,{\\bf g}\_{n})=\\frac{\\Psi\_{\\textbf{\\em or}}(p\\ |% \\ {\\bf g}\_{1},...,{\\bf g}\_{n})}{\\Psi\_{\\textbf{\\em or}}(1\\ |\\ {\\bf g}\_{1},...,{% \\bf g}\_{n})+\\Psi\_{\\textbf{\\em or}}(0\\ |\\ {\\bf g}\_{1},...,{\\bf g}\_{n})}italic\_P ( bold\_p = italic\_p | bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) = divide start\_ARG roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT ( italic\_p | bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) end\_ARG start\_ARG roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT ( 1 | bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) + roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT ( 0 | bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) end\_ARG

(32)

#### 5.6.3 The Similarity Between Disjunction and Linear Exponential

Report issue for preceding element

To underline the similarity between the linear exponential model and disjunction Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT, consider how we implement or using a log-linear model. That is, the dependence of Y𝑌Yitalic\_Y on X1subscript𝑋1X\_{1}italic\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT and X2subscript𝑋2X\_{2}italic\_X start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT can be expressed as:

Report issue for preceding element

P⁢(Y\=1|X1,X2)\=11+exp⁡(−(β0+β1⁢X1+β2⁢X2))𝑃𝑌conditional1subscript𝑋1subscript𝑋211subscript𝛽0subscript𝛽1subscript𝑋1subscript𝛽2subscript𝑋2P(Y=1|X\_{1},X\_{2})=\\frac{1}{1+\\exp(-(\\beta\_{0}+\\beta\_{1}X\_{1}+\\beta\_{2}X\_{2}))}italic\_P ( italic\_Y = 1 | italic\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_X start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT ) = divide start\_ARG 1 end\_ARG start\_ARG 1 + roman\_exp ( - ( italic\_β start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT + italic\_β start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT italic\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT + italic\_β start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT italic\_X start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT ) ) end\_ARG

(33)

If we set β0\=−0.5subscript𝛽00.5\\beta\_{0}=-0.5italic\_β start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT = - 0.5, a negative bias, and β1\=β2\=1subscript𝛽1subscript𝛽21\\beta\_{1}=\\beta\_{2}=1italic\_β start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT = italic\_β start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT = 1, then this predicts Y\=1𝑌1Y=1italic\_Y = 1 if either X1\=1subscript𝑋11X\_{1}=1italic\_X start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT = 1 or X2\=1subscript𝑋21X\_{2}=1italic\_X start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT = 1, but Y\=0𝑌0Y=0italic\_Y = 0 otherwise. That is, it implements or, and this technique scales for n\>2𝑛2n>2italic\_n > 2.

Report issue for preceding element

#### 5.6.4 On the Use of a Linear Model

Report issue for preceding element

One might ask whether it is simplistic to use a linear model for any reason when we have availble advanced networks like multi-layer networks and attention, etc. The use of non-linear networks in this context can be investigated. However, we reiterate it is the role of the conjunction gates to create the higher-level features that are accomplished currently with multi-layer networks \[[Rumelhart et al., 1986](https://arxiv.org/html/2402.06557v1#bib.bibx39)\]. Linear weights are easily interpretable, which is good for human-computer alignment. Also, for certain definitions of Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT, like the Noisy Or gate discussed in Section [9.3](https://arxiv.org/html/2402.06557v1#S9.SS3 "9.3 Faster Disjunction ‣ 9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), updates can be fast, i.e. O⁢(n)𝑂𝑛O(n)italic\_O ( italic\_n ) instead of O⁢(2n)𝑂superscript2𝑛O(2^{n})italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ), because of the independence of inputs. We leave it to future work to decide whether any O⁢(n)𝑂𝑛O(n)italic\_O ( italic\_n ) models for Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT are useful in practice.

Report issue for preceding element

6 The Implication Graph
-----------------------

Report issue for preceding element

### 6.1 Infinite Use of Finite Means

Report issue for preceding element

Chomsky was famously fond of quoting Humboldt’s aphorism that language makes infinite use of finite means \[[Chomsky, 1965](https://arxiv.org/html/2402.06557v1#bib.bibx5)\]. The implication graph allows us to estimate probabilities for an unbounded number of propositions 𝐩𝐩{\\bf p}bold\_p based on finite parameters ΨΨ\\Psiroman\_Ψ, by defining weights over predicate patterns, rather than relationships between concrete entities. That is, we learn a general link between 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT liking 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT and 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT dating 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT, and this can apply to 𝐜j⁢a⁢c⁢k⁢1subscript𝐜𝑗𝑎𝑐𝑘1{\\bf c}\_{jack1}bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k 1 end\_POSTSUBSCRIPT or 𝐜j⁢a⁢c⁢k⁢2subscript𝐜𝑗𝑎𝑐𝑘2{\\bf c}\_{jack2}bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k 2 end\_POSTSUBSCRIPT or 𝐜j⁢i⁢l⁢l⁢1subscript𝐜𝑗𝑖𝑙𝑙1{\\bf c}\_{jill1}bold\_c start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l 1 end\_POSTSUBSCRIPT or 𝐜j⁢i⁢l⁢l⁢2subscript𝐜𝑗𝑖𝑙𝑙2{\\bf c}\_{jill2}bold\_c start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l 2 end\_POSTSUBSCRIPT, etc., and so make infinite use of finite means.

Report issue for preceding element

### 6.2 Graph Operations

Report issue for preceding element

##### Construction

Report issue for preceding element

The implication graph is constructed from the set of all relevant conjoined predicate implications that we want to train weights for in our model:

Report issue for preceding element

𝒦\={Ψ⁢(𝐡,𝐪)}𝒦Ψ𝐡𝐪\\mathcal{K}=\\left\\{\\Psi({\\bf h},{\\bf q})\\right\\}caligraphic\_K = { roman\_Ψ ( bold\_h , bold\_q ) }

(34)

##### Backwards Links for a Predicate

Report issue for preceding element

From this, we can recover the backwards set of all predicate implication links for a predicate 𝐪𝐪{\\bf q}bold\_q:

Report issue for preceding element

BΨ⁢(𝐪)\={Ψ⁢(𝐡,𝐪′)∈𝒦|𝐪′\=𝐪}subscript𝐵Ψ𝐪conditional-setΨ𝐡superscript𝐪′𝒦superscript𝐪′𝐪B\_{\\Psi}({\\bf q})=\\left\\{\\Psi({\\bf h},{\\bf q}^{\\prime})\\in\\mathcal{K}\\ |\\ {\\bf q% }^{\\prime}={\\bf q}\\right\\}italic\_B start\_POSTSUBSCRIPT roman\_Ψ end\_POSTSUBSCRIPT ( bold\_q ) = { roman\_Ψ ( bold\_h , bold\_q start\_POSTSUPERSCRIPT ′ end\_POSTSUPERSCRIPT ) ∈ caligraphic\_K | bold\_q start\_POSTSUPERSCRIPT ′ end\_POSTSUPERSCRIPT = bold\_q }

(35)

It is also possible to define a forwards set but we avoid doing this and consider only BΨsubscript𝐵ΨB\_{\\Psi}italic\_B start\_POSTSUBSCRIPT roman\_Ψ end\_POSTSUBSCRIPT for simplicity.

Report issue for preceding element

### 6.3 Abstraction and Backwards Substitution

Report issue for preceding element

##### Abstraction

Report issue for preceding element

For any proposition p, whose role set is 𝐫𝐫{\\bf r}bold\_r, we can abstract any subset of the roles in 𝐫𝐫{\\bf r}bold\_r to reveal a predicate 𝐪𝐪{\\bf q}bold\_q. For example, for the proposition:

Report issue for preceding element

𝐩\=(like,{subj:𝐜j⁢a⁢c⁢k⁢1,dobj:𝐜j⁢i⁢l⁢l⁢1})𝐩likeconditional-setsubj:subscript𝐜𝑗𝑎𝑐𝑘1dobjsubscript𝐜𝑗𝑖𝑙𝑙1{\\bf p}=(\\textsc{like},\\left\\{\\textsc{subj}:{\\bf c}\_{jack1},\\textsc{dobj}:{\\bf c% }\_{jill1}\\right\\})bold\_p = ( like , { subj : bold\_c start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k 1 end\_POSTSUBSCRIPT , dobj : bold\_c start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l 1 end\_POSTSUBSCRIPT } )

(36)

Abstracting {subj,dobj}subjdobj\\left\\{\\textsc{subj},\\textsc{dobj}\\right\\}{ subj , dobj } would leave:

Report issue for preceding element

𝐪\=(like,{subj:𝐱j⁢a⁢c⁢k,dobj:𝐱j⁢i⁢l⁢l})𝐪likeconditional-setsubj:subscript𝐱𝑗𝑎𝑐𝑘dobjsubscript𝐱𝑗𝑖𝑙𝑙{\\bf q}=(\\textsc{like},\\left\\{\\textsc{subj}:{\\bf x}\_{jack},\\textsc{dobj}:{\\bf x% }\_{jill}\\right\\})bold\_q = ( like , { subj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , dobj : bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT } )

(37)

Though abstracting over all variables at once can be considered the standard abstraction, we can abstract partially in 2n−1superscript2𝑛12^{n}-12 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT - 1 different ways, as 𝐩𝐩{\\bf p}bold\_p is not included as an abstraction of itself, because it has no open roles. We will write that 𝐪∈𝐩𝐪𝐩{\\bf q}\\in{\\bf p}bold\_q ∈ bold\_p if 𝐪𝐪{\\bf q}bold\_q is an abstraction of 𝐩𝐩{\\bf p}bold\_p.

Report issue for preceding element

##### Backwards Substitution

Report issue for preceding element

Suppose that 𝐪𝐪{\\bf q}bold\_q is an abstraction of 𝐩𝐩{\\bf p}bold\_p, i.e. 𝐪∈𝐩𝐪𝐩{\\bf q}\\in{\\bf p}bold\_q ∈ bold\_p. And, suppose that Ψ⁢(𝐡,𝐪)Ψ𝐡𝐪\\Psi({\\bf h},{\\bf q})roman\_Ψ ( bold\_h , bold\_q ) is an implication link. We can define:

Report issue for preceding element

b⁢a⁢c⁢k⁢f⁢i⁢l⁢l⁢(𝐩,Ψ⁢(𝐡,𝐪))\=unique ⁢𝐠⁢ such that ⁢Ψ⁢(𝐡,𝐪)⁢ links 𝐠 to ⁢𝐩𝑏𝑎𝑐𝑘𝑓𝑖𝑙𝑙𝐩Ψ𝐡𝐪unique 𝐠 such that Ψ𝐡𝐪 links 𝐠 to 𝐩backfill({\\bf p},\\Psi({\\bf h},{\\bf q}))=\\text{unique }{\\bf g}\\text{ such that % }\\Psi({\\bf h},{\\bf q})\\text{ links ${\\bf g}$ to }{\\bf p}italic\_b italic\_a italic\_c italic\_k italic\_f italic\_i italic\_l italic\_l ( bold\_p , roman\_Ψ ( bold\_h , bold\_q ) ) = unique bold\_g such that roman\_Ψ ( bold\_h , bold\_q ) links bold\_g to bold\_p

(38)

This function can be computed because we stored the role mapping pair {r,s}𝑟𝑠\\left\\{r,s\\right\\}{ italic\_r , italic\_s } for each 𝐪a∈𝐡subscript𝐪𝑎𝐡{\\bf q}\_{a}\\in{\\bf h}bold\_q start\_POSTSUBSCRIPT italic\_a end\_POSTSUBSCRIPT ∈ bold\_h and 𝐪𝐪{\\bf q}bold\_q, for each Ψ⁢(𝐡,𝐪)Ψ𝐡𝐪\\Psi({\\bf h},{\\bf q})roman\_Ψ ( bold\_h , bold\_q ) in the implication graph.

Report issue for preceding element

### 6.4 Proposition Factors and Contexts

Report issue for preceding element

##### Proposition Factor

Report issue for preceding element

Suppose we have a proposition 𝐩𝐩{\\bf p}bold\_p, which contains the abstraction 𝐪𝐪{\\bf q}bold\_q, which matches an implication link Ψ⁢(𝐡,𝐪)Ψ𝐡𝐪\\Psi({\\bf h},{\\bf q})roman\_Ψ ( bold\_h , bold\_q ). We can call b⁢a⁢c⁢k⁢f⁢i⁢l⁢l⁢(𝐩,Ψ⁢(𝐡,𝐪))𝑏𝑎𝑐𝑘𝑓𝑖𝑙𝑙𝐩Ψ𝐡𝐪backfill({\\bf p},\\Psi({\\bf h},{\\bf q}))italic\_b italic\_a italic\_c italic\_k italic\_f italic\_i italic\_l italic\_l ( bold\_p , roman\_Ψ ( bold\_h , bold\_q ) ) to obtain some 𝐠𝐠{\\bf g}bold\_g, an instance of 𝐡𝐡{\\bf h}bold\_h, obtained by following backwards an instance of the link Ψ⁢(𝐡,𝐪)Ψ𝐡𝐪\\Psi({\\bf h},{\\bf q})roman\_Ψ ( bold\_h , bold\_q ). 𝐠\=𝐩1∧…∧𝐩n𝐠subscript𝐩1…subscript𝐩𝑛{\\bf g}={\\bf p}\_{1}\\land...\\land{\\bf p}\_{n}bold\_g = bold\_p start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT ∧ … ∧ bold\_p start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT is a conjunction of propositions, and so has a probability, unlike 𝐡𝐡{\\bf h}bold\_h, which is a predicate. These objects are all bundled up in a factor defined as:

Report issue for preceding element

f⁢a⁢c⁢t⁢o⁢r⁢(𝐩,Ψ⁢(𝐡,𝐪))\=(𝐩,Ψ⁢(𝐡,𝐪),𝐠)𝑓𝑎𝑐𝑡𝑜𝑟𝐩Ψ𝐡𝐪𝐩Ψ𝐡𝐪𝐠factor({\\bf p},\\Psi({\\bf h},{\\bf q}))=({\\bf p},\\Psi({\\bf h},{\\bf q}),{\\bf g})italic\_f italic\_a italic\_c italic\_t italic\_o italic\_r ( bold\_p , roman\_Ψ ( bold\_h , bold\_q ) ) = ( bold\_p , roman\_Ψ ( bold\_h , bold\_q ) , bold\_g )

(39)

The factor contains both the causally related proposition group 𝐠𝐠{\\bf g}bold\_g, and also the implication link Ψ⁢(𝐡,𝐪)Ψ𝐡𝐪\\Psi({\\bf h},{\\bf q})roman\_Ψ ( bold\_h , bold\_q ) used to link 𝐠𝐠{\\bf g}bold\_g and 𝐩𝐩{\\bf p}bold\_p.

Report issue for preceding element

##### Proposition Factor Context

Report issue for preceding element

For a given proposition 𝐩𝐩{\\bf p}bold\_p, its factor context is:

Report issue for preceding element

context⁢(𝐩)\=⋃𝐪∈𝐩⋃𝐡∈BΨ⁢(𝐪)f⁢a⁢c⁢t⁢o⁢r⁢(𝐩,Ψ⁢(𝐡,𝐪))context𝐩subscript𝐪𝐩subscript𝐡subscript𝐵Ψ𝐪𝑓𝑎𝑐𝑡𝑜𝑟𝐩Ψ𝐡𝐪\\textsc{context}({\\bf p})=\\bigcup\_{\\begin{subarray}{c}{\\bf q}\\in{\\bf p}\\end{% subarray}}\\ \\bigcup\_{\\begin{subarray}{c}{\\bf h}\\in B\_{\\Psi}({\\bf q})\\end{% subarray}}factor({\\bf p},\\Psi({\\bf h},{\\bf q}))context ( bold\_p ) = ⋃ start\_POSTSUBSCRIPT start\_ARG start\_ROW start\_CELL bold\_q ∈ bold\_p end\_CELL end\_ROW end\_ARG end\_POSTSUBSCRIPT ⋃ start\_POSTSUBSCRIPT start\_ARG start\_ROW start\_CELL bold\_h ∈ italic\_B start\_POSTSUBSCRIPT roman\_Ψ end\_POSTSUBSCRIPT ( bold\_q ) end\_CELL end\_ROW end\_ARG end\_POSTSUBSCRIPT italic\_f italic\_a italic\_c italic\_t italic\_o italic\_r ( bold\_p , roman\_Ψ ( bold\_h , bold\_q ) )

(40)

This is the set of all factors created from taking all backwards implication links from all abstracted predicates 𝐪∈𝐩𝐪𝐩{\\bf q}\\in{\\bf p}bold\_q ∈ bold\_p. The factor context is the input to the learned linear exponential model used to score the or gates, Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT.

Report issue for preceding element

##### Markov Assumption

Report issue for preceding element

In terms of the Markov assumption, the node 𝐩𝐩{\\bf p}bold\_p is independent of all its ancestors given its factor context. That is, the factor context contains the set of all direct causes for 𝐩𝐩{\\bf p}bold\_p, according to the current theory.

Report issue for preceding element

### 6.5 Inference-Time Proposition Graph Creation

Report issue for preceding element

When we are interested in a query 𝐩𝐩{\\bf p}bold\_p, we have to construct the graph of relevant proprositions on the fly at inference time, because we cannot store all propositions. Suppose we are interested in a certain target query 𝐩𝐩{\\bf p}bold\_p, which for simplicity for now assume has only ancestors, and no descendents in the graph. We can determine context⁢(𝐩)context𝐩\\textsc{context}({\\bf p})context ( bold\_p ), which will get all of the conjoined nodes 𝐠𝐠{\\bf g}bold\_g that are parents of 𝐩𝐩{\\bf p}bold\_p. Each 𝐠z\=𝐩z1∧…∧𝐩znsubscript𝐠𝑧subscript𝐩subscript𝑧1…subscript𝐩subscript𝑧𝑛{\\bf g}\_{z}={\\bf p}\_{z\_{1}}\\wedge...\\wedge{\\bf p}\_{z\_{n}}bold\_g start\_POSTSUBSCRIPT italic\_z end\_POSTSUBSCRIPT = bold\_p start\_POSTSUBSCRIPT italic\_z start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ∧ … ∧ bold\_p start\_POSTSUBSCRIPT italic\_z start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT is a conjunction of 𝐩zisubscript𝐩subscript𝑧𝑖{\\bf p}\_{z\_{i}}bold\_p start\_POSTSUBSCRIPT italic\_z start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT, and for each of these we can recursively call context⁢(𝐩zi)contextsubscript𝐩subscript𝑧𝑖\\textsc{context}({\\bf p}\_{z\_{i}})context ( bold\_p start\_POSTSUBSCRIPT italic\_z start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ), and so on, until we have created a proposition graph of all propositions relevant to 𝐩𝐩{\\bf p}bold\_p. Because of the Markov assumption, any node not reached through this traversal is not relevant to 𝐩𝐩{\\bf p}bold\_p. During the construction of this graph, we can do book-keeping to store, for each 𝐩𝐩{\\bf p}bold\_p and 𝐠𝐠{\\bf g}bold\_g discovered, the forward links, linking a node to its children, and backward links, linking a node to its parents, for each node of each type. We use this all at inference time, described in Section [7](https://arxiv.org/html/2402.06557v1#S7 "7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

### 6.6 Feature Function

Report issue for preceding element

The feature function ϕ⁢(𝐩,𝐠)italic-ϕ𝐩𝐠\\phi({\\bf p},{\\bf g})italic\_ϕ ( bold\_p , bold\_g ) characterizes the implication link between the conclusion 𝐩𝐩{\\bf p}bold\_p and the assumption 𝐠𝐠{\\bf g}bold\_g:

Report issue for preceding element

ϕ⁢(𝐩\=p,𝐠\=g)\=(p,Ψ⁢(𝐡,𝐪),g)italic-ϕformulae-sequence𝐩𝑝𝐠𝑔𝑝Ψ𝐡𝐪𝑔\\phi({\\bf p}=p,{\\bf g}=g)=(p,\\Psi({\\bf h},{\\bf q}),g)italic\_ϕ ( bold\_p = italic\_p , bold\_g = italic\_g ) = ( italic\_p , roman\_Ψ ( bold\_h , bold\_q ) , italic\_g )

(41)

That is, the feature ϕ⁢(𝐩\=p,𝐠\=g)italic-ϕformulae-sequence𝐩𝑝𝐠𝑔\\phi({\\bf p}=p,{\\bf g}=g)italic\_ϕ ( bold\_p = italic\_p , bold\_g = italic\_g ) is a triple indicating:

Report issue for preceding element

1.  1.
    
    The value p∈{0,1}𝑝01p\\in\\left\\{0,1\\right\\}italic\_p ∈ { 0 , 1 } that 𝐩𝐩{\\bf p}bold\_p takes in ϕ⁢(𝐩\=p,𝐠\=g)italic-ϕformulae-sequence𝐩𝑝𝐠𝑔\\phi({\\bf p}=p,{\\bf g}=g)italic\_ϕ ( bold\_p = italic\_p , bold\_g = italic\_g ).
    
    Report issue for preceding element
    
2.  2.
    
    The implication link Ψ⁢(𝐡,𝐪)Ψ𝐡𝐪\\Psi({\\bf h},{\\bf q})roman\_Ψ ( bold\_h , bold\_q ) used to arrive at 𝐩𝐩{\\bf p}bold\_p from 𝐠𝐠{\\bf g}bold\_g.
    
    Report issue for preceding element
    
3.  3.
    
    The value g∈{0,1}𝑔01g\\in\\left\\{0,1\\right\\}italic\_g ∈ { 0 , 1 } that 𝐠𝐠{\\bf g}bold\_g takes on in ϕ⁢(𝐩\=p,𝐠\=g)italic-ϕformulae-sequence𝐩𝑝𝐠𝑔\\phi({\\bf p}=p,{\\bf g}=g)italic\_ϕ ( bold\_p = italic\_p , bold\_g = italic\_g ).
    
    Report issue for preceding element
    

We usually just write ϕ⁢(𝐩,𝐠)italic-ϕ𝐩𝐠\\phi({\\bf p},{\\bf g})italic\_ϕ ( bold\_p , bold\_g ), and assume that the Ψ⁢(𝐡,𝐪)Ψ𝐡𝐪\\Psi({\\bf h},{\\bf q})roman\_Ψ ( bold\_h , bold\_q ) is implied. It is possible for the same 𝐩𝐩{\\bf p}bold\_p and 𝐠𝐠{\\bf g}bold\_g to have more than one link, which would result in more than one feature. The feature vector for the entire factor context is the union over each of the individual proposition factors.

Report issue for preceding element

7 Inference
-----------

Report issue for preceding element

### 7.1 The Probability Query

Report issue for preceding element

We are interested in the probability query, which consists of two parts:

Report issue for preceding element

*   •
    
    The query variables: a subset {𝐩}Qsubscript𝐩𝑄\\left\\{{\\bf p}\\right\\}\_{Q}{ bold\_p } start\_POSTSUBSCRIPT italic\_Q end\_POSTSUBSCRIPT of all variables in the network.
    
    Report issue for preceding element
    
*   •
    
    The evidence: a subset {𝐩}Esubscript𝐩𝐸\\left\\{{\\bf p}\\right\\}\_{E}{ bold\_p } start\_POSTSUBSCRIPT italic\_E end\_POSTSUBSCRIPT of random variables in the network, observed to have the values {p}Esubscript𝑝𝐸\\left\\{p\\right\\}\_{E}{ italic\_p } start\_POSTSUBSCRIPT italic\_E end\_POSTSUBSCRIPT.
    
    Report issue for preceding element
    

The task is to compute the posterior distribution:

Report issue for preceding element

P⁢({𝐩}Q∣{𝐩}E\={p}E)𝑃conditionalsubscript𝐩𝑄subscript𝐩𝐸subscript𝑝𝐸P(\\left\\{{\\bf p}\\right\\}\_{Q}\\mid\\left\\{{\\bf p}\\right\\}\_{E}=\\left\\{p\\right\\}\_{E})italic\_P ( { bold\_p } start\_POSTSUBSCRIPT italic\_Q end\_POSTSUBSCRIPT ∣ { bold\_p } start\_POSTSUBSCRIPT italic\_E end\_POSTSUBSCRIPT = { italic\_p } start\_POSTSUBSCRIPT italic\_E end\_POSTSUBSCRIPT )

(42)

### 7.2 Marginalization

Report issue for preceding element

In the presence of unobserved variables {𝐩}Usubscript𝐩𝑈\\left\\{{\\bf p}\\right\\}\_{U}{ bold\_p } start\_POSTSUBSCRIPT italic\_U end\_POSTSUBSCRIPT, not part of the query or evidence, marginalization is used to sum out these variables from the joint probability distribution. The marginalization process is represented by the following equation:

Report issue for preceding element

P⁢({𝐩}Q∣{𝐩}E)\=∑{𝐩}UP⁢({𝐩}Q,{𝐩}U∣{𝐩}E)𝑃conditionalsubscript𝐩𝑄subscript𝐩𝐸subscriptsubscript𝐩𝑈𝑃subscript𝐩𝑄conditionalsubscript𝐩𝑈subscript𝐩𝐸P(\\left\\{{\\bf p}\\right\\}\_{Q}\\mid\\left\\{{\\bf p}\\right\\}\_{E})=\\sum\_{\\left\\{{\\bf p% }\\right\\}\_{U}}P(\\left\\{{\\bf p}\\right\\}\_{Q},\\left\\{{\\bf p}\\right\\}\_{U}\\mid\\left% \\{{\\bf p}\\right\\}\_{E})italic\_P ( { bold\_p } start\_POSTSUBSCRIPT italic\_Q end\_POSTSUBSCRIPT ∣ { bold\_p } start\_POSTSUBSCRIPT italic\_E end\_POSTSUBSCRIPT ) = ∑ start\_POSTSUBSCRIPT { bold\_p } start\_POSTSUBSCRIPT italic\_U end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT italic\_P ( { bold\_p } start\_POSTSUBSCRIPT italic\_Q end\_POSTSUBSCRIPT , { bold\_p } start\_POSTSUBSCRIPT italic\_U end\_POSTSUBSCRIPT ∣ { bold\_p } start\_POSTSUBSCRIPT italic\_E end\_POSTSUBSCRIPT )

(43)

In general, in a Bayesian Network, this process if Ω⁢(2N)Ωsuperscript2𝑁\\Omega(2^{N})roman\_Ω ( 2 start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT ) to compute exactly, or even to provably approximate \[[Cooper, 1990](https://arxiv.org/html/2402.06557v1#bib.bibx6), [Roth, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx38)\].

Report issue for preceding element

### 7.3 Iterative Belief Propagation

Report issue for preceding element

Inference can be performed in a graphical model using belief propagation \[[Koller and Friedman, 2009](https://arxiv.org/html/2402.06557v1#bib.bibx20), [Neapolitan, 2003](https://arxiv.org/html/2402.06557v1#bib.bibx30), [Bishop, 2006](https://arxiv.org/html/2402.06557v1#bib.bibx3)\], if the graph contains even undirected cycles, which it often would, exact belief propagation is not tractable. However, empirical results suggest that loopy belief propagation, which we will call iterative belief propagation, does converge well empirically, even though there are no theoretical guarantees \[[Murphy et al., 1999](https://arxiv.org/html/2402.06557v1#bib.bibx29), [Smith and Eisner, 2008](https://arxiv.org/html/2402.06557v1#bib.bibx41)\]. We discuss the complexity of this operation in detail in Section [9](https://arxiv.org/html/2402.06557v1#S9 "9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") and our results on convergence in Section [8](https://arxiv.org/html/2402.06557v1#S8 "8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.").

Report issue for preceding element

### 7.4 Message Passing Calculations

Report issue for preceding element

##### Notation

Report issue for preceding element

We implement the variant of \[[Pearl, 1988](https://arxiv.org/html/2402.06557v1#bib.bibx31)\]’s belief propagation algorithm presented in \[[Neapolitan, 2003](https://arxiv.org/html/2402.06557v1#bib.bibx30)\]. In this formulation, we have π𝜋\\piitalic\_π values and λ𝜆\\lambdaitalic\_λ values, and π𝜋\\piitalic\_π messages and λ𝜆\\lambdaitalic\_λ messages. For factor computations, we distinguished between single propositions 𝐩𝐩{\\bf p}bold\_p and proposition groups 𝐠𝐠{\\bf g}bold\_g. However, for the message passing calculations we adopt a unified notation, where both 𝐩𝐩{\\bf p}bold\_p and 𝐠𝐠{\\bf g}bold\_g nodes can be viewed as a unified node 𝐳𝐳{\\bf z}bold\_z that can wrap either type, and the message passing calculations are agnostic to the type. We use 𝐜𝐜{\\bf c}bold\_c to canonically refer to a child of 𝐳𝐳{\\bf z}bold\_z and 𝐚𝐚{\\bf a}bold\_a for a parent of 𝐳𝐳{\\bf z}bold\_z.

Report issue for preceding element

##### Computations

Report issue for preceding element

The version we present here involves exponential cost O⁢(2n)𝑂superscript2𝑛O(2^{n})italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ) sums over either the parents or children of z𝑧zitalic\_z. In Section [9](https://arxiv.org/html/2402.06557v1#S9 "9 Complexity of Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), we discuss how the independence of Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT factors can, for some distributions like Noisy Or, allow the O⁢(2n)𝑂superscript2𝑛O(2^{n})italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ) update to be done in linear O⁢(n)𝑂𝑛O(n)italic\_O ( italic\_n ) time.

Report issue for preceding element

##### Values

Report issue for preceding element

π⁢(z)∈ℝ𝜋𝑧ℝ\\pi(z)\\in\\mathbb{R}italic\_π ( italic\_z ) ∈ blackboard\_R, called the π𝜋\\piitalic\_π value for 𝐳\=z𝐳𝑧{\\bf z}=zbold\_z = italic\_z, represents beliefs flowing forward in the network, from causes to effects, and is:

Report issue for preceding element

π⁢(z)\=∑a1,…,an(P⁢(z∣a1,…,an)⁢∏aiπ𝐳⁢(ai)).𝜋𝑧subscriptsubscript𝑎1…subscript𝑎𝑛𝑃conditional𝑧subscript𝑎1…subscript𝑎𝑛subscriptproductsubscript𝑎𝑖subscript𝜋𝐳subscript𝑎𝑖\\pi(z)=\\sum\_{a\_{1},\\ldots,a\_{n}}\\left(P(z\\mid a\_{1},\\ldots,a\_{n})\\prod\_{a\_{i}}% \\pi\_{\\bf z}(a\_{i})\\right).italic\_π ( italic\_z ) = ∑ start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , italic\_a start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( italic\_P ( italic\_z ∣ italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , italic\_a start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) ∏ start\_POSTSUBSCRIPT italic\_a start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT italic\_π start\_POSTSUBSCRIPT bold\_z end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ) ) .

(44)

λ⁢(z)∈ℝ𝜆𝑧ℝ\\lambda(z)\\in\\mathbb{R}italic\_λ ( italic\_z ) ∈ blackboard\_R, called the λ𝜆\\lambdaitalic\_λ value for 𝐳\=z𝐳𝑧{\\bf z}=zbold\_z = italic\_z, represents beliefs flowing backward in the network, from effects to causes, and is:

Report issue for preceding element

λ⁢(z)\=∏𝐜λ𝐜⁢(z)𝜆𝑧subscriptproduct𝐜subscript𝜆𝐜𝑧\\lambda(z)=\\prod\_{{\\bf c}}\\lambda\_{{\\bf c}}(z)italic\_λ ( italic\_z ) = ∏ start\_POSTSUBSCRIPT bold\_c end\_POSTSUBSCRIPT italic\_λ start\_POSTSUBSCRIPT bold\_c end\_POSTSUBSCRIPT ( italic\_z )

(45)

These two values are normalized and combined to compute the posterior probability:

Report issue for preceding element

P⁢(z|{p}E)\=α⁢λ⁢(z)⁢π⁢(z)𝑃conditional𝑧subscript𝑝𝐸𝛼𝜆𝑧𝜋𝑧P(z\\ |\\ \\left\\{p\\right\\}\_{E})=\\alpha\\lambda(z)\\pi(z)italic\_P ( italic\_z | { italic\_p } start\_POSTSUBSCRIPT italic\_E end\_POSTSUBSCRIPT ) = italic\_α italic\_λ ( italic\_z ) italic\_π ( italic\_z )

(46)

##### Messages

Report issue for preceding element

π𝐳⁢(a)∈ℝsubscript𝜋𝐳𝑎ℝ\\pi\_{\\bf z}(a)\\in\\mathbb{R}italic\_π start\_POSTSUBSCRIPT bold\_z end\_POSTSUBSCRIPT ( italic\_a ) ∈ blackboard\_R is 𝐚𝐚{\\bf a}bold\_a’s message to a child 𝐳𝐳{\\bf z}bold\_z:

Report issue for preceding element

π𝐳⁢(a)\=π⁢(a)⁢∏(𝐲∈𝐳)−𝐚λ𝐲⁢(z)subscript𝜋𝐳𝑎𝜋𝑎subscriptproduct𝐲𝐳𝐚subscript𝜆𝐲𝑧\\pi\_{\\bf z}(a)=\\pi(a)\\prod\_{({\\bf y}\\in{\\bf z})-{\\bf a}}\\lambda\_{\\bf y}(z)italic\_π start\_POSTSUBSCRIPT bold\_z end\_POSTSUBSCRIPT ( italic\_a ) = italic\_π ( italic\_a ) ∏ start\_POSTSUBSCRIPT ( bold\_y ∈ bold\_z ) - bold\_a end\_POSTSUBSCRIPT italic\_λ start\_POSTSUBSCRIPT bold\_y end\_POSTSUBSCRIPT ( italic\_z )

(47)

λ𝐜⁢(z)∈ℝsubscript𝜆𝐜𝑧ℝ\\lambda\_{\\bf c}(z)\\in\\mathbb{R}italic\_λ start\_POSTSUBSCRIPT bold\_c end\_POSTSUBSCRIPT ( italic\_z ) ∈ blackboard\_R is 𝐜𝐜{\\bf c}bold\_c’s message to a parent 𝐳𝐳{\\bf z}bold\_z, where the 𝐛isubscript𝐛𝑖{\\bf b}\_{i}bold\_b start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT are the other parents of 𝐜𝐜{\\bf c}bold\_c:

Report issue for preceding element

λ𝐜⁢(z)\=∑c\[∑b1,b2,…,bn(P⁢(c∣z,b1,b2,…,bn)⁢∏biπ𝐜⁢(bi))⁢λ⁢(c)\]subscript𝜆𝐜𝑧subscript𝑐delimited-\[\]subscriptsubscript𝑏1subscript𝑏2…subscript𝑏𝑛𝑃conditional𝑐𝑧subscript𝑏1subscript𝑏2…subscript𝑏𝑛subscriptproductsubscript𝑏𝑖subscript𝜋𝐜subscript𝑏𝑖𝜆𝑐\\lambda\_{\\bf c}(z)=\\sum\_{c}\\left\[\\sum\_{b\_{1},b\_{2},\\ldots,b\_{n}}\\left(P(c\\mid z% ,b\_{1},b\_{2},\\ldots,b\_{n})\\prod\_{b\_{i}}\\pi\_{\\bf c}(b\_{i})\\right)\\lambda(c)\\right\]italic\_λ start\_POSTSUBSCRIPT bold\_c end\_POSTSUBSCRIPT ( italic\_z ) = ∑ start\_POSTSUBSCRIPT italic\_c end\_POSTSUBSCRIPT \[ ∑ start\_POSTSUBSCRIPT italic\_b start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_b start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , … , italic\_b start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( italic\_P ( italic\_c ∣ italic\_z , italic\_b start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_b start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , … , italic\_b start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT ) ∏ start\_POSTSUBSCRIPT italic\_b start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT italic\_π start\_POSTSUBSCRIPT bold\_c end\_POSTSUBSCRIPT ( italic\_b start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ) ) italic\_λ ( italic\_c ) \]

(48)

8 Experiments
-------------

Report issue for preceding element

### 8.1 Logical Structures

Report issue for preceding element

#### 8.1.1 Method

Report issue for preceding element

##### Synthetic Data

Report issue for preceding element

We train the model with synthetic data. Our goal is to show that the QBBN can learn the model, and to investigate inference using iterative belief propagation.

Report issue for preceding element

##### Example Universe

Report issue for preceding element

We investigate the problem of of our running example in which there are two variables from a bipartite set, 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT and 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT, and we are interested whether d⁢a⁢t⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑑𝑎𝑡𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙date({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_d italic\_a italic\_t italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ). This is the problem discussed in Section [4.3.2](https://arxiv.org/html/2402.06557v1#S4.SS3.SSS2 "4.3.2 Predicate Implication Links ‣ 4.3 Quantification and Implication ‣ 4 A Novel Calculus Over Semantic Roles ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), and the graphical model for our theory of this universe is depicted in Figure [2](https://arxiv.org/html/2402.06557v1#S5.F2 "Figure 2 ‣ 5.3 Boolean Algebra ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."). For any 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT, l⁢o⁢n⁢e⁢l⁢y⁢(𝐱j⁢a⁢c⁢k)𝑙𝑜𝑛𝑒𝑙𝑦subscript𝐱𝑗𝑎𝑐𝑘lonely({\\bf x}\_{jack})italic\_l italic\_o italic\_n italic\_e italic\_l italic\_y ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) is true with probability 30%percent3030\\%30 %. For any 𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT, e⁢x⁢c⁢i⁢t⁢i⁢n⁢g⁢(𝐱j⁢i⁢l⁢l)𝑒𝑥𝑐𝑖𝑡𝑖𝑛𝑔subscript𝐱𝑗𝑖𝑙𝑙exciting({\\bf x}\_{jill})italic\_e italic\_x italic\_c italic\_i italic\_t italic\_i italic\_n italic\_g ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) is true with probability 60%percent6060\\%60 %. For any 𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jack},{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT, l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙like({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) iff l⁢o⁢n⁢e⁢l⁢y⁢(𝐱j⁢a⁢c⁢k)∨e⁢x⁢c⁢i⁢t⁢i⁢n⁢g⁢(𝐱j⁢i⁢l⁢l)𝑙𝑜𝑛𝑒𝑙𝑦subscript𝐱𝑗𝑎𝑐𝑘𝑒𝑥𝑐𝑖𝑡𝑖𝑛𝑔subscript𝐱𝑗𝑖𝑙𝑙lonely({\\bf x}\_{jack})\\lor exciting({\\bf x}\_{jill})italic\_l italic\_o italic\_n italic\_e italic\_l italic\_y ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) ∨ italic\_e italic\_x italic\_c italic\_i italic\_t italic\_i italic\_n italic\_g ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ). For any 𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jill},{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT, l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) is true is true with probability 40%percent4040\\%40 %. For any 𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢lsubscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙{\\bf x}\_{jack},{\\bf x}\_{jill}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT, d⁢a⁢t⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑑𝑎𝑡𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙date({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_d italic\_a italic\_t italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) iff l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙like({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) and l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ).

Report issue for preceding element

##### Training

Report issue for preceding element

We train on 4096 randomly generated synthetic examples. We use a very basic stochastic gradient descent implementation, in which the learning rate is fixed, without averaging. There is some error in this simplistic estimate but these experiments are primarily to check the behavior of iterative belief propagation.

Report issue for preceding element

##### Belief Propagation Convergence

Report issue for preceding element

In each case we: 1) set some evidence (possibly nothing), 2) do k𝑘kitalic\_k rounds of iterative belief propagation, where the number of rounds is plotted on the x-axis in all graphs. In all cases, iteration 00 shows the prior probability, after which we either set an observed variable or do nothing. If we set an observed variable, we then do fan out message passing from the observed variable, which involves doing lambda backward message passing up the graph from the changed node first, and then pi forward message passing back down the graph from the roots, with each fan out counting as one iteration. In this case, we see how the graph changes over iterations. If we did not set an observed variable, then we just do rounds of full forward\-backward passes, to observe that the network does not change without new information.

Report issue for preceding element

#### 8.1.2 Results

Report issue for preceding element

##### No Evidence

Report issue for preceding element

First, we investigate inference in the model for an example in which none of the variables are set. Figure [3](https://arxiv.org/html/2402.06557v1#S8.F3 "Figure 3 ‣ No Evidence ‣ 8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") shows the baseline probabilities in the model, that match the by-hand calculations we can do to verify, with some noise due to the unsophisticated gradient descent. P⁢(l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l))𝑃𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙P(like({\\bf x}\_{jack},{\\bf x}\_{jill}))italic\_P ( italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) ) is a noisy or over P⁢(l⁢o⁢n⁢e⁢l⁢y⁢(𝐱j⁢a⁢c⁢k))\=0.3𝑃𝑙𝑜𝑛𝑒𝑙𝑦subscript𝐱𝑗𝑎𝑐𝑘0.3P(lonely({\\bf x}\_{jack}))=0.3italic\_P ( italic\_l italic\_o italic\_n italic\_e italic\_l italic\_y ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) ) = 0.3 and e⁢x⁢c⁢i⁢t⁢i⁢n⁢g⁢(𝐱j⁢i⁢l⁢l)\=0.6𝑒𝑥𝑐𝑖𝑡𝑖𝑛𝑔subscript𝐱𝑗𝑖𝑙𝑙0.6exciting({\\bf x}\_{jill})=0.6italic\_e italic\_x italic\_c italic\_i italic\_t italic\_i italic\_n italic\_g ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) = 0.6 so

Report issue for preceding element

P⁢(l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)\=1)\=1−(1−0.3)⁢(1−0.6)\=0.72𝑃𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙1110.310.60.72P(like({\\bf x}\_{jack},{\\bf x}\_{jill})=1)=1-(1-0.3)(1-0.6)=0.72italic\_P ( italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) = 1 ) = 1 - ( 1 - 0.3 ) ( 1 - 0.6 ) = 0.72

In the network this is estimates as 0.780.780.780.78, which we believe is due to the noise of the gradient descent. P⁢(l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l))𝑃𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙P(like({\\bf x}\_{jack},{\\bf x}\_{jill}))italic\_P ( italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) ) and P⁢(l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k))𝑃𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘P(like({\\bf x}\_{jill},{\\bf x}\_{jack}))italic\_P ( italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) ) are independent (even in the underlying universe) so

Report issue for preceding element

P⁢(l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)∧l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k))\=P⁢(l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l))⋅P⁢(l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k))𝑃𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘⋅𝑃𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙𝑃𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘P(like({\\bf x}\_{jack},{\\bf x}\_{jill})\\land like({\\bf x}\_{jill},{\\bf x}\_{jack})% )=P(like({\\bf x}\_{jack},{\\bf x}\_{jill}))\\cdot P(like({\\bf x}\_{jill},{\\bf x}\_{% jack}))italic\_P ( italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) ∧ italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) ) = italic\_P ( italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) ) ⋅ italic\_P ( italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) )

This is 0.72⋅0.4\=0.29⋅0.720.40.290.72\\cdot 0.4=0.290.72 ⋅ 0.4 = 0.29, while the network the estimate is 0.310.310.310.31. We reiterate that we are primarily interested in the message passing in these experiments, and there are many well understood ways to improve the SGD estimate.

Report issue for preceding element

![Refer to caption](extracted/5400721/images/dating_simple_prior_plot_5.png)

Figure 3: The prior state of the network, with no observations.

Report issue for preceding element

##### Forward Only

Report issue for preceding element

In Figure [4](https://arxiv.org/html/2402.06557v1#S8.F4 "Figure 4 ‣ Forward Only ‣ 8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), we assume that l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ), which affects P⁢(d⁢a⁢t⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l))𝑃𝑑𝑎𝑡𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙P(date({\\bf x}\_{jack},{\\bf x}\_{jill}))italic\_P ( italic\_d italic\_a italic\_t italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) ), but not P⁢(l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l))𝑃𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙P(like({\\bf x}\_{jack},{\\bf x}\_{jill}))italic\_P ( italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) ), which is independent, and those so are its ancestors.

Report issue for preceding element

![Refer to caption](extracted/5400721/images/dating_simple_jill_likes_plot_5.png)

Figure 4: Assume that l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) is true. Forward inferences only.

Report issue for preceding element

##### Forward and Backward

Report issue for preceding element

In Figure [5](https://arxiv.org/html/2402.06557v1#S8.F5 "Figure 5 ‣ Forward and Backward ‣ 8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), we assume that l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙like({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ), which affects both its parents and its children (which includes many variables), but not l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ), which is independent.

Report issue for preceding element

![Refer to caption](extracted/5400721/images/dating_simple_jack_likes_plot_10.png)

Figure 5: Assume that l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙like({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) is true. Forward and backwards inference.

Report issue for preceding element

##### Backward Only

Report issue for preceding element

In Figure [6](https://arxiv.org/html/2402.06557v1#S8.F6 "Figure 6 ‣ Backward Only ‣ 8.1.2 Results ‣ 8.1 Logical Structures ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), we assume that d⁢a⁢t⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑑𝑎𝑡𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙date({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_d italic\_a italic\_t italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ), which backwards infers through the Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT gate, to l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙like({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) and l⁢i⁢k⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘like({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ). The inference of l⁢i⁢k⁢e⁢(𝐱j⁢a⁢c⁢k,𝐱j⁢i⁢l⁢l)𝑙𝑖𝑘𝑒subscript𝐱𝑗𝑎𝑐𝑘subscript𝐱𝑗𝑖𝑙𝑙like({\\bf x}\_{jack},{\\bf x}\_{jill})italic\_l italic\_i italic\_k italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) implies updated beliefs about its ancestors l⁢o⁢n⁢e⁢l⁢y⁢(𝐱j⁢a⁢c⁢k)𝑙𝑜𝑛𝑒𝑙𝑦subscript𝐱𝑗𝑎𝑐𝑘lonely({\\bf x}\_{jack})italic\_l italic\_o italic\_n italic\_e italic\_l italic\_y ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) and e⁢x⁢c⁢i⁢t⁢i⁢n⁢g⁢(𝐱j⁢i⁢l⁢l)𝑒𝑥𝑐𝑖𝑡𝑖𝑛𝑔subscript𝐱𝑗𝑖𝑙𝑙exciting({\\bf x}\_{jill})italic\_e italic\_x italic\_c italic\_i italic\_t italic\_i italic\_n italic\_g ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT ) as well.

Report issue for preceding element

![Refer to caption](extracted/5400721/images/dating_simple_they_date_plot_10.png)

Figure 6: Assume that d⁢a⁢t⁢e⁢(𝐱j⁢i⁢l⁢l,𝐱j⁢a⁢c⁢k)𝑑𝑎𝑡𝑒subscript𝐱𝑗𝑖𝑙𝑙subscript𝐱𝑗𝑎𝑐𝑘date({\\bf x}\_{jill},{\\bf x}\_{jack})italic\_d italic\_a italic\_t italic\_e ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_i italic\_l italic\_l end\_POSTSUBSCRIPT , bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) is true. Backward inferences only.

Report issue for preceding element

### 8.2 Message Propagation

Report issue for preceding element

##### Method

Report issue for preceding element

We have just seen that the QBBN with iterative belief propagation can infer over logical structures. We now ask about the propagation of beliefs over distance in the graph. To do this, we consider only a single variable 𝐱j⁢a⁢c⁢ksubscript𝐱𝑗𝑎𝑐𝑘{\\bf x}\_{jack}bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT, and a series of unary predicates α0,…,αNsubscript𝛼0…subscript𝛼𝑁\\alpha\_{0},...,\\alpha\_{N}italic\_α start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT , … , italic\_α start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT, where we use N\=10𝑁10N=10italic\_N = 10. Now α0⁢(𝐱j⁢a⁢c⁢k)subscript𝛼0subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{0}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) is determined by a 50%percent5050\\%50 % cointoss. Then, for i≥1𝑖1i\\geq 1italic\_i ≥ 1, we deterministically set αi⁢(𝐱j⁢a⁢c⁢k)\=αi−1⁢(𝐱j⁢a⁢c⁢k)subscript𝛼𝑖subscript𝐱𝑗𝑎𝑐𝑘subscript𝛼𝑖1subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{i}({\\bf x}\_{jack})=\\alpha\_{i-1}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) = italic\_α start\_POSTSUBSCRIPT italic\_i - 1 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ). That is, while α0⁢(𝐱j⁢a⁢c⁢k)subscript𝛼0subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{0}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) is random, αi⁢(𝐱j⁢a⁢c⁢k)subscript𝛼𝑖subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{i}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) for i≥1𝑖1i\\geq 1italic\_i ≥ 1 can be determined with certainty if we know the value of αi−1⁢(𝐱j⁢a⁢c⁢k)subscript𝛼𝑖1subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{i-1}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT italic\_i - 1 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) or αi+1⁢(𝐱j⁢a⁢c⁢k)subscript𝛼𝑖1subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{i+1}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT italic\_i + 1 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ). We examine how the beliefs change if we observe either α0⁢(𝐱j⁢a⁢c⁢k)subscript𝛼0subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{0}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) or αN⁢(𝐱j⁢a⁢c⁢k)subscript𝛼𝑁subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{N}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) are observed to be true.

Report issue for preceding element

##### Results

Report issue for preceding element

Figure [7](https://arxiv.org/html/2402.06557v1#S8.F7 "Figure 7 ‣ Results ‣ 8.2 Message Propagation ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") shows the prior probability of each node in the network, which is around 50%percent5050\\%50 %, with some noise, as discussed above. Figure [8](https://arxiv.org/html/2402.06557v1#S8.F8 "Figure 8 ‣ Results ‣ 8.2 Message Propagation ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") shows what happens when we observe α0⁢(𝐱j⁢a⁢c⁢k)\=1subscript𝛼0subscript𝐱𝑗𝑎𝑐𝑘1\\alpha\_{0}({\\bf x}\_{jack})=1italic\_α start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) = 1, the information propagates forward through the network in one iteration total. Figure [9](https://arxiv.org/html/2402.06557v1#S8.F9 "Figure 9 ‣ Results ‣ 8.2 Message Propagation ‣ 8 Experiments ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") shows what happens when we observe αN⁢(𝐱j⁢a⁢c⁢k)\=1subscript𝛼𝑁subscript𝐱𝑗𝑎𝑐𝑘1\\alpha\_{N}({\\bf x}\_{jack})=1italic\_α start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) = 1, the information propagates backward through the network, at a rate of one new node chaning per iteration, taking twenty iterations in total.. Note that, for each variable αi⁢(𝐱j⁢a⁢c⁢k)subscript𝛼𝑖subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{i}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ), i≥1𝑖1i\\geq 1italic\_i ≥ 1, there is also an intermediate “conjoined” node with only one element {αi−1⁢(𝐱j⁢a⁢c⁢k)}∧subscriptsubscript𝛼𝑖1subscript𝐱𝑗𝑎𝑐𝑘\\left\\{\\alpha\_{i-1}({\\bf x}\_{jack})\\right\\}\_{\\land}{ italic\_α start\_POSTSUBSCRIPT italic\_i - 1 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) } start\_POSTSUBSCRIPT ∧ end\_POSTSUBSCRIPT. We believe more efficient ways of managing belief propagation are possible than just doing repeated fan outs, but we leave this for future work.

Report issue for preceding element

![Refer to caption](extracted/5400721/images/long_chain_prior_plot_5.png)

Figure 7: This shows the prior state of the αi⁢(𝐱j⁢a⁢c⁢k)subscript𝛼𝑖subscript𝐱𝑗𝑎𝑐𝑘\\alpha\_{i}({\\bf x}\_{jack})italic\_α start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) network. Before knowing anything at all, we expect P⁢(αi⁢(𝐱j⁢a⁢c⁢k))\=0.5𝑃subscript𝛼𝑖subscript𝐱𝑗𝑎𝑐𝑘0.5P(\\alpha\_{i}({\\bf x}\_{jack}))=0.5italic\_P ( italic\_α start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) ) = 0.5 for all i𝑖iitalic\_i.

Report issue for preceding element

![Refer to caption](extracted/5400721/images/long_chain_set_0_1_plot_10.png)

Figure 8: After observing α0⁢(𝐱j⁢a⁢c⁢k)\=1subscript𝛼0subscript𝐱𝑗𝑎𝑐𝑘1\\alpha\_{0}({\\bf x}\_{jack})=1italic\_α start\_POSTSUBSCRIPT 0 end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) = 1, the beliefs propagate forward in one pass.

Report issue for preceding element

![Refer to caption](extracted/5400721/images/long_chain_set_n_1_plot_30.png)

Figure 9: After observing αN⁢(𝐱j⁢a⁢c⁢k)\=1subscript𝛼𝑁subscript𝐱𝑗𝑎𝑐𝑘1\\alpha\_{N}({\\bf x}\_{jack})=1italic\_α start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT ( bold\_x start\_POSTSUBSCRIPT italic\_j italic\_a italic\_c italic\_k end\_POSTSUBSCRIPT ) = 1, the beliefs propagate backwrd at a rate of one new node changing per iteration, and note that there are intermediate conjunction nodes, adding a constant factor to the convergence time.

Report issue for preceding element

9 Complexity of Inference
-------------------------

Report issue for preceding element

### 9.1 Provably Exact Inference

Report issue for preceding element

Inference in a general Bayesian Network is Ω⁢(2N)Ωsuperscript2𝑁\\Omega(2^{N})roman\_Ω ( 2 start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT ) for N𝑁Nitalic\_N variables, and is even Ω⁢(2N)Ωsuperscript2𝑁\\Omega(2^{N})roman\_Ω ( 2 start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT ) to provably approximate \[[Cooper, 1990](https://arxiv.org/html/2402.06557v1#bib.bibx6), [Roth, 1996](https://arxiv.org/html/2402.06557v1#bib.bibx38)\].

Report issue for preceding element

### 9.2 Empirically Successful Iterative Belief Propagation

Report issue for preceding element

The iterative belief propagation (loopy belief propagation in the literature) is not guaranteed to converge \[[Neapolitan, 2003](https://arxiv.org/html/2402.06557v1#bib.bibx30), [Koller and Friedman, 2009](https://arxiv.org/html/2402.06557v1#bib.bibx20)\], but has been found to converge in practice in a range of studies \[[Smith and Eisner, 2008](https://arxiv.org/html/2402.06557v1#bib.bibx41), [Murphy et al., 1999](https://arxiv.org/html/2402.06557v1#bib.bibx29), [Gormley et al., 2015](https://arxiv.org/html/2402.06557v1#bib.bibx16)\]. And, we have found it to converge in our experiments. The primary cost of inference in this case is the computation of the messages and values of the π𝜋\\piitalic\_π and λ𝜆\\lambdaitalic\_λ tables (see Section [7](https://arxiv.org/html/2402.06557v1#S7 "7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")) Using a perhaps naive implementation, in which the marginalization is exact (see [44](https://arxiv.org/html/2402.06557v1#S7.E44 "44 ‣ Values ‣ 7.4 Message Passing Calculations ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.") and [48](https://arxiv.org/html/2402.06557v1#S7.E48 "48 ‣ Messages ‣ 7.4 Message Passing Calculations ‣ 7 Inference ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations.")), runs in time O⁢(2n)𝑂superscript2𝑛O(2^{n})italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ) in n𝑛nitalic\_n the number of inputs to the factor. Then, a single pass of belief propagation visits each of the N𝑁Nitalic\_N nodes once, taking total time O⁢(N⁢2n)𝑂𝑁superscript2𝑛O(N2^{n})italic\_O ( italic\_N 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ), and empirically k𝑘kitalic\_k rounds are needed to converge. However, it may be possible to make both Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT and Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT gates faster, as we now discuss.

Report issue for preceding element

### 9.3 Faster Disjunction

Report issue for preceding element

##### Overview

Report issue for preceding element

The Ψ𝒐𝒓subscriptΨ𝒐𝒓\\Psi\_{\\textbf{\\em or}}roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT factor is learned when we want to do statistical inference, and the factor

Report issue for preceding element

Ψ𝒐𝒓⁢(𝐩|𝐠1,…,𝐠n)subscriptΨ𝒐𝒓conditional𝐩subscript𝐠1…subscript𝐠𝑛\\Psi\_{\\textbf{\\em or}}({\\bf p}\\ |\\ {\\bf g}\_{1},...,{\\bf g}\_{n})roman\_Ψ start\_POSTSUBSCRIPT or end\_POSTSUBSCRIPT ( bold\_p | bold\_g start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , … , bold\_g start\_POSTSUBSCRIPT italic\_n end\_POSTSUBSCRIPT )

has one input n𝑛nitalic\_n per modeled cause 𝐠isubscript𝐠𝑖{\\bf g}\_{i}bold\_g start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT of 𝐩𝐩{\\bf p}bold\_p. Conceptually, outcomes have an unbounded number of potential causes, and we would ideally not need to restrict n𝑛nitalic\_n solely because of message passing complexity.

Report issue for preceding element

##### Importance Sampling

Report issue for preceding element

One option is to learn an unbounded number n𝑛nitalic\_n of weights, but only consider at inference a subset of the inputs that are most relevant. That is, in the linear exponential model [31](https://arxiv.org/html/2402.06557v1#S5.E31 "31 ‣ 5.6.2 Learned Disjunctive Model ‣ 5.6 Disjunction Nodes ‣ 5 The Proposition Graph ‣ The Quantified Boolean Bayesian Network Theory and Experiments with a Logical Graphical Model The author acknowledges the use of ChatGPT in the production of this work, for research, proof-reading, and the production of equations."), we can detect which of m<n𝑚𝑛m<nitalic\_m < italic\_n linear contributions will have the biggest effect, and only marginalize over those, costing O⁢(2m)<O⁢(2n)𝑂superscript2𝑚𝑂superscript2𝑛O(2^{m})<O(2^{n})italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_m end\_POSTSUPERSCRIPT ) < italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ). This strategy would be a variant of importance sampling \[[Wilkinson, 2005](https://arxiv.org/html/2402.06557v1#bib.bibx50)\].

Report issue for preceding element

##### Linear Time Disjunction

Report issue for preceding element

\[[Neapolitan, 2003](https://arxiv.org/html/2402.06557v1#bib.bibx30)\] lists at least one disjunction model, the Noisy Or model, whose message passing calculations are O⁢(n)𝑂𝑛O(n)italic\_O ( italic\_n ), instead of O⁢(2n)𝑂superscript2𝑛O(2^{n})italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ) in n𝑛nitalic\_n the number of inputs to the factor. We leave it to future work to determine whether this model, or another model with similar scaling properties, can be useful in practice.

Report issue for preceding element

### 9.4 Faster Conjunction

Report issue for preceding element

The complexity of message updates in a conjunction Ψ𝒂𝒏𝒅subscriptΨ𝒂𝒏𝒅\\Psi\_{\\textbf{\\em and}}roman\_Ψ start\_POSTSUBSCRIPT and end\_POSTSUBSCRIPT gate is O⁢(2n)𝑂superscript2𝑛O(2^{n})italic\_O ( 2 start\_POSTSUPERSCRIPT italic\_n end\_POSTSUPERSCRIPT ) in the number of inputs n𝑛nitalic\_n. However, an and gate can be arranged into a binary tree of and gates each of size 2222, with the tree height log2⁡(n)subscript2𝑛\\log\_{2}(n)roman\_log start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT ( italic\_n ), in which case there would be only O⁢(n)𝑂𝑛O(n)italic\_O ( italic\_n ) work in total to evaluate the n𝑛nitalic\_n inputs. However, this would increase the amount of message passing, so we leave it to future work to evaluate whether this is beneficial.

Report issue for preceding element

10 Compared to Other Logical Models
-----------------------------------

Report issue for preceding element

##### AlphaGeometry

Report issue for preceding element

\[[Trinh et al., 2024](https://arxiv.org/html/2402.06557v1#bib.bibx48)\] present a model that is trained to solve problems from the geometry olympiad. We observe that such questions are purely mathematical, and so can be solved by ordinary deterministic theorem provers. So, the use of LLM’s would seem to us a potential efficiency improvement in the field of automatic theorem-proving, which is definitely an interesting direction to consider. Our work focuses instead on the general relationship between logical and statistical reasoning. Our analysis of simple versus complex kinds of inferences \[[Coppola, 2024a](https://arxiv.org/html/2402.06557v1#bib.bibx7)\], shows that there is a difference between everyday reasoning, and the kinds of complex mathematical reasoning that is necessary for mathematics: specifically, in complex proofs, the assumptions change, and this requires some kind of book-keeping or resoning by cases. This shows why we may not want to do full theorem-proving for typical information retrieval, but instead focus on only those inferences that are fast. Also, our work is meant to apply to all language, and to analyze the logical and probabilistic nature of all language. Also, our analysis of the QBBN in terms of a complete and consistent calculus on the basis of \[[Prawitz, 1965](https://arxiv.org/html/2402.06557v1#bib.bibx34)\] allows us to understand what logical rules are implemented now, versus what can be implemented, and thus provides a clear, principled program for further research.

Report issue for preceding element

##### Self-Discover

Report issue for preceding element

\[[Zhou et al., 2024](https://arxiv.org/html/2402.06557v1#bib.bibx52)\] uses a technique called self-discover to learn what the authors say is cause-and-effect reasoning. This work seems to rely on modules of computation, that seem to be complex but not listed. Also, it is not clear how this method would be extended to ensure logical or probabilistic consistency in a graph containing one variable per possible proposition, which is an enormous and indeed unbounded number. There are no complex modules in our work, all of the equations are given here. Also, our use of the well-studied system of Bayesian Networks \[[Pearl, 1988](https://arxiv.org/html/2402.06557v1#bib.bibx31)\], gives us a framework for understanding consistency that is non-trivial and contains many useful results and theorems.

Report issue for preceding element

11 Implementation
-----------------

Report issue for preceding element

An implementation of the QBBN called BAYES STAR can be found at \[[Coppola, 2024b](https://arxiv.org/html/2402.06557v1#bib.bibx8)\]. This implementation is written in the Rust programming language, with REDIS for storage, and includes the code for training and doing inference in each of the examples discussed here.

Report issue for preceding element

12 Future Work
--------------

Report issue for preceding element

##### Learning from Unlabeled Text

Report issue for preceding element

We have said that the QBBN can encode knowledge, and do so without hallucinating, which compares favorably with the LLM \[[Bahdanau et al., 2014](https://arxiv.org/html/2402.06557v1#bib.bibx1), [Vaswani et al., 2017](https://arxiv.org/html/2402.06557v1#bib.bibx49), [Radford et al., 2018](https://arxiv.org/html/2402.06557v1#bib.bibx35)\]. However, the difficulty compared to the LLM is that the QBBN cannot be learned in the same direct n𝑛nitalic\_n\-gram language model way as the LLM, but instead must refer to logical forms which are not observed but viewed as latent and must be learned through expectation maximization \[[Dempster et al., 1977](https://arxiv.org/html/2402.06557v1#bib.bibx9)\].

Report issue for preceding element

##### Belief Propagation

Report issue for preceding element

We have used loopy belief propagation, calling it iterative belief propagation, which is not guaranteed to converge but has been studied somewhat extensively \[[Murphy et al., 1999](https://arxiv.org/html/2402.06557v1#bib.bibx29), [Smith and Eisner, 2008](https://arxiv.org/html/2402.06557v1#bib.bibx41), [Gormley et al., 2015](https://arxiv.org/html/2402.06557v1#bib.bibx16)\], and our experiments also find convergence. However, convergence for larger graphs should be studied, as well as strategies to speed up belief propagation.

Report issue for preceding element

##### Logical Language Features

Report issue for preceding element

We have shown enough about the underlying logical language of the QBBN to encode basic first-order sentences. But, there remain the topics of compositional semantics \[[Montague, 1970](https://arxiv.org/html/2402.06557v1#bib.bibx27)\], which shows how the meanings of larger parts are made from smaller parts, and intensional semantics \[[Montague, 1973](https://arxiv.org/html/2402.06557v1#bib.bibx28)\], which shows how the concept behind a sentence can itself be an argument.

Report issue for preceding element

References
----------

Report issue for preceding element

*   \[Bahdanau et al., 2014\]↑ Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. CoRR, abs/1409.0473.
*   \[Bar-Hillel, 1953\]↑ Bar-Hillel, Y. (1953). A Quasi-Arithmetical Notation for Syntactic Description. Language, 29(1):47–58.
*   \[Bishop, 2006\]↑ Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
*   \[Chomsky, 1957\]↑ Chomsky, N. (1957). Syntactic Structures. Mouton, The Hague.
*   \[Chomsky, 1965\]↑ Chomsky, N. (1965). Aspects of the Theory of Syntax. MIT Press, Cambridge, MA. Available online: [https://mitpress.mit.edu](https://mitpress.mit.edu).
*   \[Cooper, 1990\]↑ Cooper, G. F. (1990). The Computational Complexity of Probabilistic Inference Using Bayesian Belief Networks. Artificial Intelligence, 42(2-3):393–405.
*   \[Coppola, 2024a\]↑ Coppola, G. (2024a). A Mathematical Explanation for “Thinking Fast and Slow”. Bitcoin Ordinal NFT 72494446539c7fcb73becde763fc4bbbf0686b9c30cd8188e50861ccde0a5c83i0.
*   \[Coppola, 2024b\]↑ Coppola, G. (2024b). Bayes-Star: An Implementation of a Quantified Boolean Bayesian Network. [https://github.com/gregorycoppola/bayes-star](https://github.com/gregorycoppola/bayes-star).
*   \[Dempster et al., 1977\]↑ Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1–38.
*   \[Devlin et al., 2018\]↑ Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
*   \[Eisner, 1996\]↑ Eisner, J. (1996). Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th conference on Computational linguistics-Volume 1, pages 340–345. Association for Computational Linguistics.
*   \[Frege, 1879\]↑ Frege, G. (1879). Begriffsschrift, a Formula Language, Modeled Upon That of Arithmetic, for Pure Thought. Friedrich Frommann Verlag (Günther Holzboog).
*   \[Gentzen, 1934\]↑ Gentzen, G. (1934). Untersuchungen über das logische schließen. Mathematische Zeitschrift, 39:176–210, 405–431.
*   \[Gilks et al., 1996\]↑ Gilks, W. R., Richardson, S., and Spiegelhalter, D. J., editors (1996). Markov Chain Monte Carlo in Practice. Chapman and Hall/CRC.
*   \[Gödel, 1930\]↑ Gödel, K. (1930). On the completeness of the calculus of logic. Monatshefte für Mathematik und Physik, 37:349–360.
*   \[Gormley et al., 2015\]↑ Gormley, M. R., Dredze, M., and Eisner, J. (2015). Approximation-aware dependency parsing by belief propagation. Transactions of the Association for Computational Linguistics, 3:489–501.
*   \[Hinton, 2023\]↑ Hinton, G. (2023). “Godfather of Artificial Intelligence” talks impact and potential of AI. CBS Mornings, YouTube. Accessed: 2023-12-26, Timestamp: 1318 seconds.
*   \[Hinton et al., 2015\]↑ Hinton, G. E., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network. NIPS Deep Learning and Representation Learning Workshop.
*   \[Kahneman, 2011\]↑ Kahneman, D. (2011). Thinking, Fast and Slow. Farrar, Straus and Giroux, New York.
*   \[Koller and Friedman, 2009\]↑ Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. MIT Press.
*   \[LeCun, 2023\]↑ LeCun, Y. (2023). From Machine Learning to Autonomous Intelligence. Ludwig-Maximilians-Universität München, YouTube. Accessed: 2024-01-29.
*   \[LeCun et al., 1989\]↑ LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1989). Handwritten digit recognition: Applications of neural networks. In NeurIPS.
*   \[Lewis and Steedman, 2013\]↑ Lewis, M. and Steedman, M. (2013). Combined distributional and logical semantics. Transactions of the Association for Computational Linguistics, 1:179–192.
*   \[Lundberg and Lee, 2017\]↑ Lundberg, S. M. and Lee, S.-I. (2017). A unified approach to interpreting model predictions. In NeurIPS.
*   \[McDonald et al., 2005\]↑ McDonald, R., Pereira, F., Ribarov, K., and Hajič, J. (2005). Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523–530.
*   \[Minsky and Papert, 1969\]↑ Minsky, M. and Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. MIT Press.
*   \[Montague, 1970\]↑ Montague, R. (1970). Universal grammar. Theoria, 36:373–398. Reprinted in Thomason, Richmond H. (ed.), Formal Philosophy: Selected Papers of Richard Montague, pp. 7–27, Yale University Press, 1974.
*   \[Montague, 1973\]↑ Montague, R. (1973). The Proper Treatment of Quantification in Ordinary English. Approaches to Natural Language, pages 221–242.
*   \[Murphy et al., 1999\]↑ Murphy, K., Weiss, Y., and Jordan, M. I. (1999). Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999), pages 467–476. AUAI.
*   \[Neapolitan, 2003\]↑ Neapolitan, R. E. (2003). Learning Bayesian Networks. Prentice Hall.
*   \[Pearl, 1988\]↑ Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.
*   \[Pelletier, 2000\]↑ Pelletier, F. J. (2000). A history of natural deduction and elementary logic textbooks. Logical consequence: Rival approaches, 1:105–138.
*   \[Petrov et al., 2006\]↑ Petrov, S., Barrett, L., Thibaux, R., and Klein, D. (2006). Learning accurate, compact, and interpretable tree annotation. In Association for Computational Linguistics, pages 433–440. Association for Computational Linguistics.
*   \[Prawitz, 1965\]↑ Prawitz, D. (1965). Natural Deduction: A Proof-Theoretical Study. Stockholm Studies in Philosophy 3. Almqvist & Wiksell, Stockholm; Göteborg; Uppsala. Acta Universitatis Stockholmiensis.
*   \[Radford et al., 2018\]↑ Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training.
*   \[Ribeiro et al., 2016\]↑ Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). "why should i trust you?" explaining the predictions of any classifier. In KDD, pages 1135–1144.
*   \[Richardson and Domingos, 2006\]↑ Richardson, M. and Domingos, P. (2006). Markov logic networks. Machine learning, 62:107–136.
*   \[Roth, 1996\]↑ Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82:273–302.
*   \[Rumelhart et al., 1986\]↑ Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323:533–536.
*   \[Shannon, 1948\]↑ Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27:379–423, 623–656.
*   \[Smith and Eisner, 2008\]↑ Smith, D. and Eisner, J. (2008). Dependency parsing by belief propagation. In Lapata, M. and Ng, H. T., editors, Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 145–156, Honolulu, Hawaii. Association for Computational Linguistics.
*   \[Steedman, 1996\]↑ Steedman, M. (1996). Surface Structure and Interpretation. The MIT Press.
*   \[Steedman, 2022\]↑ Steedman, M. (2022). 2022 NLP Symposium. AI Quorum, YouTube. Accessed: 2023-12-29, Timestamp: 7 seconds.
*   \[Sutskever, 2023\]↑ Sutskever, I. (2023). An observation on generalization. Simons Institute, YouTube. Accessed: 2024-01-29.
*   \[Sutskever and Huang, 2023\]↑ Sutskever, I. and Huang, J. (2023). AI Today and Vision of the Future. 999,999 Views, YouTube. Accessed: 2023-12-26, Timestamp: 1966 seconds.
*   \[Sutskever et al., 2014\]↑ Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems, 27.
*   \[Sutton and McCallum, 2011\]↑ Sutton, C. and McCallum, A. (2011). An Introduction to Conditional Random Fields. Foundations and Trends in Machine Learning, 4(4):267–373.
*   \[Trinh et al., 2024\]↑ Trinh, T. H., Wu, Y., Le, Q. V., He, H., and Luong, T. (2024). Solving olympiad geometry without human demonstrations. Nature, 625:476–482.
*   \[Vaswani et al., 2017\]↑ Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems, volume 30.
*   \[Wilkinson, 2005\]↑ Wilkinson, L. (2005). The Grammar of Graphics. Springer, 2 edition.
*   \[Zhang and Nivre, 2011\]↑ Zhang, Y. and Nivre, J. (2011). Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193.
*   \[Zhou et al., 2024\]↑ Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.-T., Le, Q. V., Chi, E. H., Zhou, D., Mishra, S., and Zheng, H. S. (2024). Self-discover: Large language models self-compose reasoning structures.

Generated by [L A T E xml ![[LOGO]](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==)](https://math.nist.gov/~BMiller/LaTeXML/) 

Instructions for reporting errors
---------------------------------

We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:

*   Click the "Report Issue" button.
*   Open a report feedback form via keyboard, use "**Ctrl + ?**".
*   Make a text selection and click the "Report Issue for Selection" button near your cursor.
*   You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.

Our team has already identified [the following issues](https://github.com/arXiv/html_feedback/issues). We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.

Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a [list of packages that need conversion](https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML), and welcome [developer contributions](https://github.com/brucemiller/LaTeXML/issues).

Report Issue

##### Report Github Issue

Title:Content selection saved. Describe the issue below:Description:

Submit without GithubSubmit in Github

Report Issue for Selection